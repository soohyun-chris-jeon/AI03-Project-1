import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from torch.utils.data import DataLoader, Dataset # PyTorchì˜ Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ê¸° ìœ„í•œ ëª¨ë“ˆ
import matplotlib.patches as patches
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
import os              # íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json            # JSON íŒŒì¼ì„ ì½ê³  ì“°ê¸° ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch           # ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ PyTorch (í…ì„œ ì—°ì‚° ë“±)
from PIL import Image  # ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬
from torchvision import transforms
from pathlib import Path
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.optim import Adam
import torch.nn as nn
import matplotlib
from pathlib import Path  # payhon path
import albumentations as A
import cv2  # OpenCV - ê³ ê¸‰ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì²˜ë¦¬
import pandas as pd
from tqdm import tqdm
from torchmetrics.detection.mean_ap import MeanAveragePrecision
# Garbage Collector ëª¨ë“ˆ
import gc


# (1) ì‹œìŠ¤í…œë³„ í°íŠ¸
import platform
if platform.system() == 'Darwin':  # Mac
    font_name = 'AppleGothic'

matplotlib.rc('font', family=font_name)
# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False


# ë©”ëª¨ë¦¬ ì •ë¦¬ ë£¨í‹´
gc.collect()
torch.cuda.empty_cache()


# --- ë””ë°”ì´ìŠ¤ ì„¤ì • ---
DEVICE = torch.device("cpu") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {DEVICE}")
SEED = 42



# --- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
BATCH_SIZE = 16
NUM_EPOCHS = 10
LEARNING_RATE = 1e-4
MOMENTUM = 0.9
WEIGHT_DECAY = 0.0005

# --- ë°ì´í„° ê²½ë¡œ ì„¤ì • ---
DATA_ROOT = "/Users/jogyeseung/Desktop/AI03-Project-1/data/raw"  # ë³¸ì¸ ë°ì´í„°ì˜ ì ˆëŒ€ê²½ë¡œ
TRAIN_IMAGE_DIR = os.path.join(DATA_ROOT, "train_images")
TRAIN_ANNO_DIR = os.path.join(DATA_ROOT, "train_annotations")
TEST_IMAGE_DIR = os.path.join(DATA_ROOT, "test_images")
# PROCESSED_TRAIN_CSV = "../data/processed/train_df.csv"  # ë°ì´í„° ì „ì²˜ë¦¬ëœ csv íŒŒì¼

# --- ëª¨ë¸ ì„¤ì • ---
NUM_CLASSES = 73
MODEL_NAME = "fasterrcnn_resnet50_fpn"
USE_PRETRAINED = True

# --- í•™ìŠµ ê³ ë„í™” ì„¤ì • ---
USE_SCHEDULER = True  # Learning rate scheduler ì‚¬ìš© ì—¬ë¶€
EARLY_STOPPING = True  # Early stopping ì ìš© ì—¬ë¶€
AUGMENTATION = True  # ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€

# --- ì‹¤í—˜ ë¡œê¹…ìš© ì„¤ì • ---
USE_WANDB = True
WANDB_PROJECT = "AI03-Project-1"
RUN_NAME = f"{MODEL_NAME}_bs{BATCH_SIZE}_lr{LEARNING_RATE}"


# --- ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ---
EXPERIMENT_DIR = "/Users/jogyeseung/Desktop/AI03-Project-1/experiments"



def parse_raw_annotations(ann_dir: Path) -> pd.DataFrame:
    """
    ë³µì¡í•œ 3ì¤‘ í´ë” êµ¬ì¡°ì˜ ì›ë³¸ ì–´ë…¸í…Œì´ì…˜ì„ íŒŒì‹±í•˜ì—¬
    í•˜ë‚˜ì˜ Pandas DataFrameìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    all_annotations = []

    # Level 1: ì´ë¯¸ì§€ë³„ í´ë” ìˆœíšŒ
    image_level_dirs = os.listdir(ann_dir)
    for image_dir_name in tqdm(image_level_dirs, desc="[L1] Images"):
        image_dir_path = ann_dir / image_dir_name
        if not image_dir_path.is_dir():
            continue

        # Level 2: ì•Œì•½ ì¢…ë¥˜ í´ë” ìˆœíšŒ
        pill_level_dirs = os.listdir(image_dir_path)
        for pill_dir_name in pill_level_dirs:
            pill_dir_path = image_dir_path / pill_dir_name
            if not pill_dir_path.is_dir():
                continue

            # Level 3: ì‹¤ì œ .json íŒŒì¼ íŒŒì‹±
            json_files = [f for f in os.listdir(pill_dir_path) if f.endswith(".json")]
            if not json_files:
                continue

            # ì²« ë²ˆì§¸ json íŒŒì¼ë§Œ ì‚¬ìš©
            json_file_path = pill_dir_path / json_files[0]

            try:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    ann_data = json.load(f)

                    image_info = ann_data.get("images", [{}])[0]
                    annotation_info = ann_data.get("annotations", [{}])[0]
                    category_info = ann_data.get("categories", [{}])[0]

                    all_annotations.append(
                        {
                            "image_id": image_info.get("id"),
                            "file_name": image_info.get("file_name"),
                            "width": image_info.get("width"),
                            "height": image_info.get("height"),
                            "category_id": category_info.get("id"),
                            "class_name": category_info.get("name"),
                            "bbox": annotation_info.get("bbox"),
                        }
                    )
            except Exception as e:
                print(f"\níŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {json_file_path}, ì—ëŸ¬: {e}")

    return pd.DataFrame(all_annotations)

# 1. í•µì‹¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ì„œ DataFrame ìƒì„±
from pathlib import Path

# --- ë°ì´í„° ê²½ë¡œ ì„¤ì • ---
DATA_ROOT = "/Users/jogyeseung/Desktop/AI03-Project-1/data/raw"
TRAIN_IMAGE_DIR = Path(DATA_ROOT) / "train_images"
TRAIN_ANNO_DIR = Path(DATA_ROOT) / "train_annotations"
TEST_IMAGE_DIR = Path(DATA_ROOT) / "test_images"

# --- ë°ì´í„° ì „ì²˜ë¦¬ëœ csv íŒŒì¼ ì €ì¥ ê²½ë¡œ ---
PROCESSED_DATA_DIR = Path("/Users/jogyeseung/Desktop/AI03-Project-1/data/processed")
SAVE_PATH = PROCESSED_DATA_DIR / "train_df.csv"
# --- í•µì‹¬ í•¨ìˆ˜ í˜¸ì¶œí•´ì„œ DataFrame ìƒì„± ---
train_df = parse_raw_annotations(TRAIN_ANNO_DIR)


# --- (1). bbox ì»¬ëŸ¼ì„ 4ê°œë¡œ ë¶„ë¦¬ ---
# bbox ì»¬ëŸ¼ ë¶„ë¦¬
bbox_df = pd.DataFrame(
    train_df["bbox"].tolist(), columns=["bbox_x", "bbox_y", "bbox_w", "bbox_h"]
)
train_df = pd.concat([train_df.drop("bbox", axis=1), bbox_df], axis=1)

# âœ¨ --- [í•µì‹¬ ìˆ˜ì •] ì˜ëª»ëœ ë°”ìš´ë”© ë°•ìŠ¤ ë°ì´í„° ì œê±° ---
# xmax (bbox_x + bbox_w)ê°€ ì´ë¯¸ì§€ ë„ˆë¹„(width)ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
invalid_x = train_df["bbox_x"] + train_df["bbox_w"] > train_df["width"]
# ymax (bbox_y + bbox_h)ê°€ ì´ë¯¸ì§€ ë†’ì´(height)ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
invalid_y = train_df["bbox_y"] + train_df["bbox_h"] > train_df["height"]

# ì˜ëª»ëœ ë°ì´í„°ë¥¼ í•„í„°ë§
invalid_rows = train_df[invalid_x | invalid_y]
if not invalid_rows.empty:
    print(f"--- {len(invalid_rows)}ê°œì˜ ì˜ëª»ëœ ë°”ìš´ë”© ë°•ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ---")
    print(
        invalid_rows[
            ["file_name", "width", "height", "bbox_x", "bbox_y", "bbox_w", "bbox_h"]
        ]
    )

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ë‚¨ê¹€
    train_df = train_df[~(invalid_x | invalid_y)]
    print(f"\nì˜ëª»ëœ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³ , {len(train_df)}ê°œì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# --- (2). category_idë¥¼ ìƒˆë¡œìš´ label_idxë¡œ ë§¤í•‘ ---
# ê³ ìœ í•œ category_id ëª©ë¡ì„ ë½‘ì•„ ì •ë ¬
unique_category_ids = sorted(train_df["category_id"].unique())
NUM_CLASSES = len(unique_category_ids)
# category_idë¥¼ 0, 1, 2... ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
id_to_idx = {
    int(original_id): idx
    for idx, original_id in enumerate(
        unique_category_ids, start=1
    )  # <--- start=1 ì¶”ê°€!
}
# ì´ ë§¤í•‘ ì •ë³´ë¥¼ ì‚¬ìš©í•´ì„œ 'label_idx'ë¼ëŠ” ìƒˆ ì»¬ëŸ¼ì„ ì¶”ê°€
train_df["label_idx"] = train_df["category_id"].map(id_to_idx)

# ë‚˜ì¤‘ì— ì¶”ë¡  ê²°ê³¼ì—ì„œ ì›ë˜ í´ë˜ìŠ¤ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë§¤í•‘ ì •ë³´ë„ ì €ì¥
label_map = {
    "id_to_idx": id_to_idx,
    "idx_to_id": {idx: int(original_id) for original_id, idx in id_to_idx.items()},
    "id_to_name": dict(zip(train_df["category_id"], train_df["class_name"])),
}
with open(PROCESSED_DATA_DIR / "label_map.json", "w", encoding="utf-8") as f:
    json.dump(label_map, f, ensure_ascii=False, indent=4)

print(f"\nì´ {len(unique_category_ids)}ê°œì˜ ê³ ìœ  í´ë˜ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
print("ë¼ë²¨ ë§¤í•‘ ì •ë³´ë¥¼ 'data/processed/label_map.json'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# 3. ìµœì¢… DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥
train_df.to_csv(SAVE_PATH, index=False)

print(f"\n--- ë°ì´í„° ì „ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ! ---")
print(train_df.head())

print(train_df["label_idx"].value_counts().plot(kind="bar"))

print(train_df["class_name"].value_counts())

classes_tmp = train_df["class_name"].unique()
print(classes_tmp)

# 1. 'category_id'ì™€ 'class_name' ì»¬ëŸ¼ìœ¼ë¡œ ê³ ìœ í•œ ìŒì„ ì°¾ê³ , ID ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
class_mapping_df = (
    train_df[["category_id", "class_name"]]
    .drop_duplicates()
    .sort_values(by="category_id")
)
# 2. ì •ë ¬ëœ DataFrameì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
sorted_class_names = [
    name.split("(")[0].strip() for name in class_mapping_df["class_name"]
]
# 3. ë§¨ ì•ì— 'background' ì¶”ê°€
classes = ["background"] + sorted_class_names

print(f"ì´ í´ë˜ìŠ¤ ê°œìˆ˜ (ë°°ê²½ í¬í•¨): {len(classes)}")
print(f"í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸: {classes}")

# Load a model
# (1) ë°ì´í„° ì¦ê°• (Augmentation) : Albumentations ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
train_transforms = A.Compose(
    [
        A.Resize(512, 512),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.2),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        # PyTorch í…ì„œë¡œ ë³€í™˜
        A.pytorch.ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format="albumentations", label_fields=["labels"]),
)  # bbox í˜•ì‹ì€ pascal_voc: [xmin, ymin, xmax, ymax]

val_transforms = A.Compose(
    [
        A.Resize(512, 512),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        A.pytorch.ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format="albumentations", label_fields=["labels"]),
)

test_transforms = A.Compose(
    [
        A.Resize(512, 512),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        A.pytorch.ToTensorV2(),
    ]
)


# DataLoaderë¥¼ ìœ„í•œ collate_fn. ì´ë¯¸ì§€ì™€ íƒ€ê²Ÿì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ì¤Œ
def collate_fn(batch):
    return tuple(zip(*batch))

class PillDataset(Dataset):
    # --- mode íŒŒë¼ë¯¸í„° ì¶”ê°€ ë° dfë¥¼ ì§ì ‘ ë°›ë„ë¡ ìˆ˜ì • ---
    def __init__(self, df, image_dir, mode="train", transforms=None):
        self.df = df
        self.image_dir = Path(image_dir)
        self.mode = mode
        self.transforms = transforms

        # --- image_idsë¥¼ ë¯¸ë¦¬ ë½‘ì•„ ì¤‘ë³µì„ ì œê±° ---
        # df['file_name']ì„ ì‚¬ìš©í•˜ë©´ ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ê³ ìœ í•œ ì´ë¯¸ì§€ë¥¼ ì‹ë³„ ê°€ëŠ¥.
        self.image_ids = self.df["file_name"].unique()

    def __len__(self):
        # --- ê³ ìœ í•œ ì´ë¯¸ì§€ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜ ---
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        image_path = self.image_dir / image_id

        image = cv2.imread(str(image_path))
        if image is None:
            raise FileNotFoundError(
                f"Error: Could not load image at path: {image_path}"
            )

        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w, _ = image.shape

        if self.mode in ["train", "val"]:
            records = self.df[self.df["file_name"] == image_id]
            boxes = records[["bbox_x", "bbox_y", "bbox_w", "bbox_h"]].values

            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]
            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]

            labels = records["label_idx"].values

            # print(f"\n[DEBUG 1] Image: {image_id}, Original Pixel Coords:\n{boxes}")

            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì •ê·œí™”
            boxes = boxes.astype(np.float32)
            boxes[:, [0, 2]] /= w
            boxes[:, [1, 3]] /= h

            # print(f"[DEBUG 2] Normalized Coords for Albumentations:\n{boxes}")

            if self.transforms:
                try:
                    transformed = self.transforms(
                        image=image, bboxes=boxes, labels=labels
                    )
                    image = transformed["image"]
                    boxes = transformed["bboxes"]
                    labels = transformed["labels"]
                except Exception as e:
                    print(f"!!!!!!!!!!!!!! Albumentationsì—ì„œ ì—ëŸ¬ ë°œìƒ !!!!!!!!!!!!!!")
                    print(f"Image: {image_id}")
                    print(f"Boxes sent to transform: {boxes}")
                    # raise e  # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ë©ˆì¶”ê²Œ í•¨

            # ... ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼ ...
            _, new_h, new_w = image.shape
            boxes = torch.as_tensor(boxes, dtype=torch.float32)

            if len(boxes) > 0:
                boxes[:, [0, 2]] *= new_w
                boxes[:, [1, 3]] *= new_h

            target = {
                "boxes": boxes,
                "labels": torch.as_tensor(labels, dtype=torch.int64),
            }

            if len(target["boxes"]) == 0:
                target["boxes"] = torch.zeros((0, 4), dtype=torch.float32)
                target["labels"] = torch.zeros((0,), dtype=torch.int64)

            return image, target

            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš°, ì´ë¯¸ì§€ì™€ íŒŒì¼ ì´ë¦„ë§Œ ë°˜í™˜
        elif self.mode == "test":
            # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ë³´í†µ ê¸°ë³¸ì ì¸ ë¦¬ì‚¬ì´ì¦ˆ, ì •ê·œí™”ë§Œ ì ìš©
            if self.transforms:
                transformed = self.transforms(image=image)
                image = transformed["image"]

            # ë‚˜ì¤‘ì— ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì™€ ë§¤ì¹­ì‹œí‚¤ê¸° ìœ„í•´ íŒŒì¼ ì´ë¦„ì„ ë°˜í™˜
            return image, image_id


# ì°¸ê³ : Subsetì„ ì‚¬ìš©í•  ë•Œ transformì„ ë‹¤ë¥´ê²Œ ì ìš©í•˜ë ¤ë©´ ì•½ê°„ì˜ íŠ¸ë¦­ì´ í•„ìš”.
# ë¨¼ì € transformì´ ì—†ëŠ” ì „ì²´ ë°ì´í„°ì…‹ì„ ë§Œë“¦.
# ê° Subsetì— ë§ëŠ” transformì„ ì ìš©í•˜ëŠ” Wrapper í´ë˜ìŠ¤ ìƒì„±
# class TransformSubset(Dataset):
#     def __init__(self, subset, transforms):
#         self.subset = subset
#         self.transforms = transforms

#     def __getitem__(self, idx):
#         image, target = self.subset[idx]

#         # NumPy ë°°ì—´ë¡œ ë³€í™˜ (Albumentations ì…ë ¥ í˜•ì‹)
#         boxes = target["boxes"].numpy()
#         labels = target["labels"].numpy()

#         if self.transforms:
#             transformed = self.transforms(image=image, bboxes=boxes, labels=labels)
#             image = transformed["image"]
#             target["boxes"] = torch.as_tensor(
#                 transformed["bboxes"], dtype=torch.float32
#             )
#             # ì¦ê°• í›„ bboxê°€ ì‚¬ë¼ì¡Œì„ ê²½ìš° ì²˜ë¦¬
#             if len(target["boxes"]) == 0:
#                 target["boxes"] = torch.zeros((0, 4), dtype=torch.float32)

#         return image, target

#     def __len__(self):
#         return len(self.subset)

import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# pre-trained ëª¨ë¸ ë¡œë“œ
# Faster R-CNN
# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")

# MobileNetV3
model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(
    weights="DEFAULT"
)


# ë¶„ë¥˜ê¸°ì˜ ì…ë ¥ í”¼ì²˜ ìˆ˜ë¥¼ ê°€ì ¸ì˜´
in_features = model.roi_heads.box_predictor.cls_score.in_features

# pre-trained headë¥¼ ìƒˆë¡œìš´ headë¡œ êµì²´
# num_classesì— ë°°ê²½(background) í´ë˜ìŠ¤ 1ê°œë¥¼ ë”í•´ì¤˜ì•¼ í•¨
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES + 1)

import random


# ì¬í˜„ì„±ì„ ìœ„í•´ ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸°ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜.
def seed_everything(seed: int):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"Seed set to {seed}")

    from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedGroupKFold
from torch.utils.data import Subset

seed_everything(SEED)

# 1. ë°ì´í„° ì¤€ë¹„
df = pd.read_csv(SAVE_PATH)

# StratifiedGroupKFoldë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
groups = df["file_name"]  # ê·¸ë£¹ ê¸°ì¤€: ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„
labels = df["category_id"]  # ì¸µí™” ê¸°ì¤€: ì›ë³¸ í´ë˜ìŠ¤ ID

# K-Fold ì„¤ì • (5-fold, ì¦‰ 80% train / 20% val)
cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)

# ì²« ë²ˆì§¸ foldì˜ train/val ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´
train_idxs, val_idxs = next(cv.split(df, labels, groups))
# 1. ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„°í”„ë ˆì„ì„ ë¨¼ì € ë¶„í• !
train_df_split = df.iloc[train_idxs].reset_index(drop=True)
val_df_split = df.iloc[val_idxs].reset_index(drop=True)

# 2. ë¶„í• ëœ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê°ê° Dataset ìƒì„± (Subset, TransformSubset ë¶ˆí•„ìš”!)
train_dataset = PillDataset(
    df=train_df_split,
    image_dir=TRAIN_IMAGE_DIR,
    mode="train",
    transforms=train_transforms,
)

val_dataset = PillDataset(
    df=val_df_split,
    image_dir=TRAIN_IMAGE_DIR,
    mode="val",
    transforms=val_transforms,  # val_transforms ì‚¬ìš©
)


test_df = pd.DataFrame({"file_name": os.listdir(TEST_IMAGE_DIR)})

test_dataset = PillDataset(
    df=test_df,
    image_dir=TEST_IMAGE_DIR,
    mode="test",
    transforms=test_transforms,
)


# --- Data Loader ---
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
)
val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
)
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
)

img, image_id = next(iter(val_loader))
image_id[3]

model = model.to(DEVICE)

params = [p for p in model.parameters() if p.requires_grad]

# optimizer = torch.optim.SGD(
#     params,
#     lr=LEARNING_RATE,
#     momentum=MOMENTUM,
#     weight_decay=WEIGHT_DECAY,
# )
optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=0.01)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=NUM_EPOCHS, eta_min=1e-6
)

for name, param in model.named_parameters():
    print(f"{name}: requires_grad={param.requires_grad}")

# 3. í•™ìŠµ ë£¨í”„
print("--- Start Training ---")
metric = MeanAveragePrecision(box_format="xyxy").to(DEVICE)
# early_stopping = EarlyStopping(patience=7, verbose=True, path=path_model)
train_losses = []
val_losses = []
for epoch in range(NUM_EPOCHS):
    # ===================================
    #  Training Step
    # ===================================

    model.train()
    running_loss = 0.0
    loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{NUM_EPOCHS}]")

    for images, targets in loop:
        images = list(image.to(DEVICE) for image in images)
        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        running_loss += losses.item()
    avg_train_loss = running_loss / len(train_loader)

    scheduler.step()

    # =====================================
    #  Validation Step (âœ¨ ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì •)
    # =====================================
    model.eval()
    val_loss = 0.0
    metric.reset()
    # (2) Validation phase
    for images, targets in val_loader:
        images = [img.to(DEVICE) for img in images]
        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]
        # 1. mAP ê³„ì‚°ì„ ìœ„í•œ ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”)
        with torch.no_grad():  # ğŸ‘ˆâœ¨ ì˜ˆì¸¡ ë¶€ë¶„ë§Œ no_gradë¡œ ê°ì‹¸ê¸°
            predictions = model(images)

        # 2. Metric ì—…ë°ì´íŠ¸
        metric.update(predictions, targets)

        # 3. Validation Loss ê³„ì‚° (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° í•„ìš”)
        #    torch.no_grad() ë¸”ë¡ ë°”ê¹¥ì—ì„œ ê³„ì‚°
        model.train()  # Loss ê³„ì‚°ì„ ìœ„í•´ ì ì‹œ train ëª¨ë“œë¡œ
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        val_loss += losses.item()
        model.eval()  # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ ë‹¤ì‹œ eval ëª¨ë“œë¡œ ë³µê·€

    average_val_loss = val_loss / len(val_loader)
    val_losses.append(average_val_loss)

    # mAP í‰ê°€
    mAP_dict = metric.compute()
    # mAP = evaluate_model(all_predictions, all_ground_truths, classes)
    print(
        f"Train Loss: {avg_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, Validation mAP: {mAP_dict['map_50']:.4f}"
    )

print("--- Finish Training ---"),
# ìµœì¢… ëª¨ë¸ ì €ì¥
os.makedirs(EXPERIMENT_DIR, exist_ok=True)
torch.save(model.state_dict(), f"{EXPERIMENT_DIR}/final_model.pt")

print(EXPERIMENT_DIR)