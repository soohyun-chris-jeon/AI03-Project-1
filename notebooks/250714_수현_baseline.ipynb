{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3652fe3",
   "metadata": {},
   "source": [
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c49bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# STEP 0: Import Libraries\n",
    "# ====================================================================\n",
    "# 과제에 사용할 library들을 import\n",
    "\n",
    "# 시스템 및 입출력 관련\n",
    "import os  # 디렉토리, 파일 경로 조작 등\n",
    "from PIL import Image  # 이미지 열기 및 처리 (Pillow)\n",
    "from tqdm import tqdm  # 반복문의 진행 상태 시각화\n",
    "from pathlib import Path  # payhon path\n",
    "\n",
    "\n",
    "# 시각화 도구\n",
    "import matplotlib.pyplot as plt  # 기본 시각화 라이브러리\n",
    "import seaborn as sns  # 고급 시각화 (히트맵, 스타일 등)\n",
    "\n",
    "# 이미지 처리\n",
    "import cv2  # OpenCV - 고급 이미지/비디오 처리\n",
    "\n",
    "# 수치 연산\n",
    "import numpy as np  # 배열, 벡터 계산 등\n",
    "\n",
    "# PyTorch 기본 구성\n",
    "import torch  # 텐서, 연산 등\n",
    "import torch.nn as nn  # 모델 정의 (layer, loss 등)\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Optimizer (SGD, Adam 등)\n",
    "\n",
    "# PyTorch 데이터 처리\n",
    "from torch.utils.data import Dataset, DataLoader  # 커스텀 데이터셋, 배치 로딩\n",
    "\n",
    "# PyTorch 이미지 전처리\n",
    "import torchvision\n",
    "from torchvision import transforms  # 기본 이미지 transform\n",
    "from torchvision import datasets  # torchvision 내장 데이터셋\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision.transforms import v2  # torchvision v2 transforms (최신 API)\n",
    "\n",
    "# 싸이킷런 평가 지표\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 싸이킷런 데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 통계 tool\n",
    "import pandas as pd\n",
    "\n",
    "# 실험 추적 및 하이퍼파라미터 관리\n",
    "import wandb  # Weights & Biases - 실험 로깅, 시각화, 하이퍼파라미터 튜닝\n",
    "\n",
    "# Garbage Collector 모듈\n",
    "import gc\n",
    "\n",
    "# Data Augmentation 패키지: Albumentations\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bfea0",
   "metadata": {},
   "source": [
    "## 1. Set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab778e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# STEP 1: Configuration 설정\n",
    "# ====================================================================\n",
    "# 하이퍼파라미터 및 경로 등 실험에 필요한 설정들을 모아둠\n",
    "# 실험 추적 및 재현성을 위해 모든 값은 여기에서 수정하고자 함\n",
    "\n",
    "# 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 주요 하이퍼파라미터\n",
    "LEARNING_RATE = 1e-4  # 학습률 (optimizer용)\n",
    "BATCH_SIZE = 16  # 배치 크기\n",
    "NUM_EPOCHS = 100  # 학습 epoch 수\n",
    "SEED = 42  # 재현성을 위한 random seed\n",
    "\n",
    "# 데이터 경로 설정\n",
    "# DATA_ROOT = path\n",
    "# train_dir = os.path.join(DATA_ROOT, \"train\")\n",
    "# val_dir = os.path.join(DATA_ROOT, \"val\")\n",
    "# test_dir = os.path.join(DATA_ROOT, \"test\")\n",
    "\n",
    "# 모델 설정\n",
    "MODEL_NAME = \"PROJECT1_baseline\"  # 또는 \"EfficientNet\", 등등\n",
    "USE_PRETRAINED = True  # torchvision 모델 사용 여부\n",
    "\n",
    "# 학습 고도화 설정 (Optional)\n",
    "USE_SCHEDULER = True  # Learning rate scheduler 사용 여부\n",
    "EARLY_STOPPING = True  # Early stopping 적용 여부\n",
    "AUGMENTATION = True  # 데이터 증강 사용 여부\n",
    "\n",
    "# 실험 로깅용 설정\n",
    "USE_WANDB = True\n",
    "WANDB_PROJECT = \"cats-and-dogs-breeds-classification-oxford-dataset\"\n",
    "RUN_NAME = f\"{MODEL_NAME}_bs{BATCH_SIZE}_lr{LEARNING_RATE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e29721",
   "metadata": {},
   "source": [
    "## 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1425639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\\n\",\n",
    "# STEP 2: Data Pre-processing\\n\",\n",
    "# ====================================================================\\n\",\n",
    "\n",
    "# 데이터셋 경로 (수정 필요)\n",
    "DATA_ROOT = \"../data/raw/ai03-level1-project/\"  # 예시 경로\n",
    "train_image_dir = os.path.join(DATA_ROOT, \"train_images\")\n",
    "train_ann_path = os.path.join(DATA_ROOT, \"train_annotations\")\n",
    "test_image_dir = os.path.join(DATA_ROOT, \"test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43836210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw/ai03-level1-project/train_images'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5785c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class PillDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_path, transforms=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # 어노테이션 파일 로드\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "            \n",
    "        self.image_info = self.annotations['images']\n",
    "        self.annotation_info = self.annotations['annotations']\n",
    "        self.category_info = self.annotations['categories']\n",
    "        \n",
    "        # 이미지 ID를 키로 어노테이션을 빠르게 찾기 위한 딕셔너리\n",
    "        self.image_id_to_anns = {}\n",
    "        for ann in self.annotation_info:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.image_id_to_anns:\n",
    "                self.image_id_to_anns[img_id] = []\n",
    "            self.image_id_to_anns[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 정보 가져오기\n",
    "        img_info = self.image_info[idx]\n",
    "        image_path = self.image_dir / img_info['file_name']\n",
    "        \n",
    "        # 이미지 로드 (Albumentations는 BGR 순서를 사용)\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 해당 이미지의 모든 어노테이션 가져오기\n",
    "        image_id = img_info['id']\n",
    "        anns = self.image_id_to_anns.get(image_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # bbox: [x, y, w, h] -> [xmin, ymin, xmax, ymax]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # torchvision 모델이 요구하는 target 형태\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        # Albumentations 변환 적용\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=target['boxes'], labels=target['labels'])\n",
    "            image = transformed['image']\n",
    "            # Albumentations는 bbox 형식을 유지하므로 변환 필요 없음\n",
    "            target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "            # 만약 bbox가 augmentation 후 사라지면 에러가 날 수 있으므로 처리\n",
    "            if len(target['boxes']) == 0:\n",
    "                # 임의의 작은 박스를 넣어주거나, 이 이미지를 스킵하는 로직이 필요\n",
    "                # 여기서는 간단하게 크기가 0인 텐서를 만듦\n",
    "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ca451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 증강 (Augmentation) 정의\n",
    "# Albumentations를 사용하는 것이 바운딩 박스 변환에 더 유리해\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    # PyTorch 텐서로 변환\n",
    "    A.pytorch.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])) # bbox 형식은 pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.pytorch.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "# train_dataset = PillDataset(train_image_dir, train_ann_path, transforms=train_transforms)\n",
    "# val_dataset = PillDataset(val_image_dir, val_ann_path, transforms=val_transforms)\n",
    "\n",
    "# DataLoader를 위한 collate_fn. 이미지와 타겟을 리스트로 묶어줌\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# 데이터 로더 생성 (실제 데이터셋 경로 설정 후 주석 해제)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# print(\"DataLoaders are ready.\")\n",
    "# print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "# print(f\"Number of validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61a8d7",
   "metadata": {},
   "source": [
    "## 3. Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26c3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21bc158",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b287fe",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55c038",
   "metadata": {},
   "source": [
    "## 6. Results & Disscussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4ed03",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
