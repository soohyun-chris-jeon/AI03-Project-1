{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab59fb0",
   "metadata": {},
   "source": [
    "Pytoch의 dataLoader에 Sampling 기능을 추가한 버전(25.07.28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ee864",
   "metadata": {},
   "source": [
    "## 0. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de11a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# Configuration 설정\n",
    "# ====================================================================\n",
    "# 하이퍼파라미터 및 경로 등 실험에 필요한 설정들을 모아둠\n",
    "# 실험 추적 및 재현성을 위해 모든 값은 여기에서 수정하고자 함\n",
    "import os  # 디렉토리, 파일 경로 조작 등\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path  # payhon path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import cv2  # OpenCV - 고급 이미지/비디오 처리\n",
    "import torch\n",
    "from torch.utils.data import Dataset  # 커스텀 데이터셋, 배치 로딩\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Garbage Collector 모듈\n",
    "import gc\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.ops import box_iou  # 맨 위에 import 추가\n",
    "\n",
    "\n",
    "# 메모리 정리 루틴\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- 디바이스 설정 ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# --- 학습 하이퍼파라미터 ---\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 60\n",
    "LEARNING_RATE = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# --- 데이터 경로 설정 ---\n",
    "# DATA_ROOT = path\n",
    "# DATA_ROOT = \"data/raw/ai03-level1-project/\"  # 경로\n",
    "# TRAIN_IMAGE_DIR = os.path.join(DATA_ROOT, \"train_images\")\n",
    "# TRAIN_ANNO_DIR = os.path.join(DATA_ROOT, \"train_annotations\")\n",
    "# TEST_IMAGE_DIR = os.path.join(DATA_ROOT, \"test_images\")\n",
    "# PROCESSED_TRAIN_CSV = \"../data/processed/train_df.csv\"  # 데이터 전처리된 csv 파일\n",
    "\n",
    "# --- 모델 설정 ---\n",
    "NUM_CLASSES = 73\n",
    "MODEL_NAME = \"BASELINE_fasterrcnn_Mobilev3_WeightedSampler\"\n",
    "USE_PRETRAINED = True\n",
    "\n",
    "# --- 학습 고도화 설정 ---\n",
    "USE_SCHEDULER = True  # Learning rate scheduler 사용 여부\n",
    "EARLY_STOPPING = True  # Early stopping 적용 여부\n",
    "AUGMENTATION = True  # 데이터 증강 사용 여부\n",
    "\n",
    "# --- 실험 로깅용 설정 ---\n",
    "USE_WANDB = True\n",
    "WANDB_PROJECT = \"AI03-Project-1\"\n",
    "RUN_NAME = f\"{MODEL_NAME}_bs{BATCH_SIZE}_lr{LEARNING_RATE}\"\n",
    "\n",
    "\n",
    "# --- 실험 결과 저장 경로 ---\n",
    "EXPERIMENT_DIR = \"../experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ac65f",
   "metadata": {},
   "source": [
    "## 1. Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85626da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_annotations(ann_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    복잡한 3중 폴더 구조의 원본 어노테이션을 파싱하여\n",
    "    하나의 Pandas DataFrame으로 반환하는 함수.\n",
    "    \"\"\"\n",
    "    all_annotations = []\n",
    "\n",
    "    # Level 1: 이미지별 폴더 순회\n",
    "    image_level_dirs = os.listdir(ann_dir)\n",
    "    for image_dir_name in tqdm(image_level_dirs, desc=\"[L1] Images\"):\n",
    "        image_dir_path = ann_dir / image_dir_name\n",
    "        if not image_dir_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        # Level 2: 알약 종류 폴더 순회\n",
    "        pill_level_dirs = os.listdir(image_dir_path)\n",
    "        for pill_dir_name in pill_level_dirs:\n",
    "            pill_dir_path = image_dir_path / pill_dir_name\n",
    "            if not pill_dir_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Level 3: 실제 .json 파일 파싱\n",
    "            json_files = [f for f in os.listdir(pill_dir_path) if f.endswith(\".json\")]\n",
    "            if not json_files:\n",
    "                continue\n",
    "\n",
    "            # 첫 번째 json 파일만 사용\n",
    "            json_file_path = pill_dir_path / json_files[0]\n",
    "\n",
    "            try:\n",
    "                with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    ann_data = json.load(f)\n",
    "\n",
    "                    image_info = ann_data.get(\"images\", [{}])[0]\n",
    "                    annotation_info = ann_data.get(\"annotations\", [{}])[0]\n",
    "                    category_info = ann_data.get(\"categories\", [{}])[0]\n",
    "\n",
    "                    all_annotations.append(\n",
    "                        {\n",
    "                            \"image_id\": image_info.get(\"id\"),\n",
    "                            \"file_name\": image_info.get(\"file_name\"),\n",
    "                            \"width\": image_info.get(\"width\"),\n",
    "                            \"height\": image_info.get(\"height\"),\n",
    "                            \"category_id\": category_info.get(\"id\"),\n",
    "                            \"class_name\": category_info.get(\"name\"),\n",
    "                            \"bbox\": annotation_info.get(\"bbox\"),\n",
    "                        }\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n파일 처리 에러: {json_file_path}, 에러: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e807cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[L1] Images:   0%|          | 0/498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[L1] Images: 100%|██████████| 498/498 [00:01<00:00, 269.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 73개의 고유 클래스를 발견했습니다.\n",
      "라벨 매핑 정보를 'data/processed/label_map.json'에 저장했습니다.\n",
      "\n",
      "--- 데이터 전처리 및 저장 완료! ---\n",
      "   image_id                                          file_name  width  height  \\\n",
      "0       895  K-003544-010221-016551-021026_0_2_0_2_75_000_2...    976    1280   \n",
      "1       896  K-003544-010221-016551-021026_0_2_0_2_90_000_2...    976    1280   \n",
      "2       895  K-003544-010221-016551-021026_0_2_0_2_75_000_2...    976    1280   \n",
      "3       895  K-003544-010221-016551-021026_0_2_0_2_75_000_2...    976    1280   \n",
      "4      1310  K-003483-027733-028763-029667_0_2_0_2_90_000_2...    976    1280   \n",
      "\n",
      "   category_id         class_name  bbox_x  bbox_y  bbox_w  bbox_h  label_idx  \n",
      "0        10220         쎄로켈정 100mg     584     774     227     223         14  \n",
      "1         3543  무코스타정(레바미피드)(비매품)     110     234     207     200          5  \n",
      "2        21025       펠루비정(펠루비프로펜)     101     821     178     180         36  \n",
      "3        16550      동아가바펜틴정 800mg     534      42     296     458         24  \n",
      "4        28762       트라젠타정(리나글립틴)     194     195     212     202         54  \n"
     ]
    }
   ],
   "source": [
    "# 1. 핵심 함수를 호출해서 DataFrame 생성\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"../\")  # 상대경로\n",
    "RAW_DATA_DIR = BASE_DIR / \"data/raw/ai03-level1-project\"\n",
    "PROCESSED_DATA_DIR = BASE_DIR / \"data/processed\"\n",
    "\n",
    "TRAIN_IMAGE_DIR = RAW_DATA_DIR / \"train_images\"\n",
    "TRAIN_ANNO_DIR = RAW_DATA_DIR / \"train_annotations\"\n",
    "TEST_IMAGE_DIR = RAW_DATA_DIR / \"test_images\"\n",
    "SAVE_PATH = PROCESSED_DATA_DIR / \"train_df.csv\"\n",
    "train_df = parse_raw_annotations(TRAIN_ANNO_DIR)\n",
    "\n",
    "# --- (1). bbox 컬럼을 4개로 분리 ---\n",
    "# bbox 컬럼 분리\n",
    "bbox_df = pd.DataFrame(\n",
    "    train_df[\"bbox\"].tolist(), columns=[\"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]\n",
    ")\n",
    "train_df = pd.concat([train_df.drop(\"bbox\", axis=1), bbox_df], axis=1)\n",
    "\n",
    "# ✨ --- [핵심 수정] 잘못된 바운딩 박스 데이터 제거 ---\n",
    "# xmax (bbox_x + bbox_w)가 이미지 너비(width)를 초과하는 경우\n",
    "invalid_x = train_df[\"bbox_x\"] + train_df[\"bbox_w\"] > train_df[\"width\"]\n",
    "# ymax (bbox_y + bbox_h)가 이미지 높이(height)를 초과하는 경우\n",
    "invalid_y = train_df[\"bbox_y\"] + train_df[\"bbox_h\"] > train_df[\"height\"]\n",
    "\n",
    "# 잘못된 데이터를 필터링\n",
    "invalid_rows = train_df[invalid_x | invalid_y]\n",
    "if not invalid_rows.empty:\n",
    "    print(f\"--- {len(invalid_rows)}개의 잘못된 바운딩 박스 데이터를 찾았습니다. ---\")\n",
    "    print(\n",
    "        invalid_rows[\n",
    "            [\"file_name\", \"width\", \"height\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 유효한 데이터만 남김\n",
    "    train_df = train_df[~(invalid_x | invalid_y)]\n",
    "    print(f\"\\n잘못된 데이터를 제거하고, {len(train_df)}개의 데이터만 사용합니다.\")\n",
    "\n",
    "# --- (2). category_id를 새로운 label_idx로 매핑 ---\n",
    "# 고유한 category_id 목록을 뽑아 정렬\n",
    "unique_category_ids = sorted(train_df[\"category_id\"].unique())\n",
    "NUM_CLASSES = len(unique_category_ids)\n",
    "# category_id를 0, 1, 2... 인덱스로 변환하는 딕셔너리 생성\n",
    "id_to_idx = {\n",
    "    int(original_id): idx\n",
    "    for idx, original_id in enumerate(\n",
    "        unique_category_ids, start=1\n",
    "    )  # <--- start=1 추가!\n",
    "}\n",
    "# 이 매핑 정보를 사용해서 'label_idx'라는 새 컬럼을 추가\n",
    "train_df[\"label_idx\"] = train_df[\"category_id\"].map(id_to_idx)\n",
    "\n",
    "# processed 폴더 존재 여부 체크\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 나중에 추론 결과에서 원래 클래스 이름을 찾을 수 있도록 매핑 정보도 저장\n",
    "label_map = {\n",
    "    \"id_to_idx\": id_to_idx,\n",
    "    \"idx_to_id\": {idx: int(original_id) for original_id, idx in id_to_idx.items()},\n",
    "    \"id_to_name\": dict(zip(train_df[\"category_id\"], train_df[\"class_name\"])),\n",
    "}\n",
    "with open(PROCESSED_DATA_DIR / \"label_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n총 {len(unique_category_ids)}개의 고유 클래스를 발견했습니다.\")\n",
    "print(\"라벨 매핑 정보를 'data/processed/label_map.json'에 저장했습니다.\")\n",
    "\n",
    "\n",
    "# 3. 최종 DataFrame을 CSV 파일로 저장\n",
    "train_df.to_csv(SAVE_PATH, index=False)\n",
    "\n",
    "print(f\"\\n--- 데이터 전처리 및 저장 완료! ---\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190b5136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label_idx'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAG0CAYAAADzdmcjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATDpJREFUeJzt3Xl8E2X+B/DPpGnTtE1DD9o0NLRVytlyCIgUgYJcXaRyyCEqVBFQEWUBkWNd6gUVOWR1lxUPUEFBXUAUFFHuxYNDruJRBASkBUVsKWKp9Pv7g838Mp0ktNDKgJ/36zWvVzPPzOSZ65lPJpOniogIiIiIiAzKdLkrQEREROQPwwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERma+XJX4GKUlZXh6NGjsNlsUBTlcleHiIiIKkBEcOrUKTidTphMFb9fckWGlaNHj8Llcl3uahAREdFFOHz4MOLj4ys8/RUZVmw2G4DzKxseHn6Za0NEREQVUVRUBJfLpV7HK+qKDCvur37Cw8MZVoiIiK4wlX2Egw/YEhERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoZkvdwUuVeL4FerfB3O6X8aaEBERUXXgnRUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMrRKhZWpU6eiZcuWsNlsiImJQc+ePfHNN99ophERZGdnw+l0wmq1Ij09Hbm5uZppSkpKMHLkSERHRyM0NBSZmZk4cuTIpa8NERERXXUqFVbWr1+PESNG4LPPPsPq1avx+++/o0uXLjh9+rQ6zbRp0zBz5kw8//zz2LJlCxwOBzp37oxTp06p04waNQpLly7FokWLsGnTJhQXF+Pmm2/GuXPnqm7NiIiI6KqgiIhc7Mw//vgjYmJisH79erRr1w4iAqfTiVGjRuGRRx4BcP4uSmxsLJ5++mkMHz4chYWFqFmzJl5//XX0798fAHD06FG4XC6sXLkSXbt2veD7FhUVwW63o7CwEI2nbFTHs58VIiIi4/K8foeHh1d4vkt6ZqWwsBAAEBkZCQA4cOAACgoK0KVLF3Uai8WC9u3bY/PmzQCAbdu2obS0VDON0+lESkqKOk15JSUlKCoq0gxERET053DRYUVEMHr0aNx4441ISUkBABQUFAAAYmNjNdPGxsaqZQUFBQgKCkJERITPacqbOnUq7Ha7OrhcroutNhEREV1hLjqsPPDAA9i1axfefPNNXZmiKJrXIqIbV56/aSZMmIDCwkJ1OHz48MVWm4iIiK4wFxVWRo4cieXLl2Pt2rWIj49XxzscDgDQ3SE5fvy4erfF4XDg7NmzOHnypM9pyrNYLAgPD9cMRERE9OdQqbAiInjggQewZMkSrFmzBklJSZrypKQkOBwOrF69Wh139uxZrF+/HmlpaQCA5s2bIzAwUDNNfn4+9uzZo05DRERE5Fap/7o8YsQIvPHGG3j33Xdhs9nUOyh2ux1WqxWKomDUqFGYMmUKkpOTkZycjClTpiAkJAQDBw5Upx0yZAjGjBmDqKgoREZGYuzYsUhNTUWnTp2qfg2JiIjoilapsDJnzhwAQHp6umb8vHnzkJWVBQAYN24czpw5g/vvvx8nT55Eq1at8NFHH8Fms6nTz5o1C2azGf369cOZM2dw0003Yf78+QgICLi0tSEiIqKrziX1s3K5sJ8VIiKiK89l6WeFiIiIqLoxrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhVTqsbNiwAT169IDT6YSiKFi2bJmmXFEUr8MzzzyjTpOenq4rHzBgwCWvDBEREV19Kh1WTp8+jSZNmuD555/3Wp6fn68ZXnnlFSiKgj59+mimGzp0qGa6F1544eLWgIiIiK5q5srOkJGRgYyMDJ/lDodD8/rdd99Fhw4dcM0112jGh4SE6KYlIiIiKq9an1k5duwYVqxYgSFDhujKFi5ciOjoaDRq1Ahjx47FqVOnfC6npKQERUVFmoGIiIj+HCp9Z6UyXn31VdhsNvTu3Vsz/vbbb0dSUhIcDgf27NmDCRMmYOfOnVi9erXX5UydOhWPPfZYdVaViIiIDKpaw8orr7yC22+/HcHBwZrxQ4cOVf9OSUlBcnIyWrRoge3bt+O6667TLWfChAkYPXq0+rqoqAgul6v6Kk5ERESGUW1hZePGjfjmm2+wePHiC0573XXXITAwEHl5eV7DisVigcViqY5qEhERkcFV2zMrL7/8Mpo3b44mTZpccNrc3FyUlpYiLi6uuqpDREREV6hK31kpLi7Gvn371NcHDhzAjh07EBkZidq1awM4/zXN22+/jRkzZujm/+6777Bw4UL85S9/QXR0NPbu3YsxY8agWbNmaNOmzSWsChEREV2NKh1Wtm7dig4dOqiv3c+SDB48GPPnzwcALFq0CCKC2267TTd/UFAQPvnkE8yePRvFxcVwuVzo3r07Jk+ejICAgItcDSIiIrpaKSIil7sSlVVUVAS73Y7CwkI0nrJRHX8wp/tlrBURERH543n9Dg8Pr/B8/N9AREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRoDCtERERkaAwrREREZGgMK0RERGRolQ4rGzZsQI8ePeB0OqEoCpYtW6Ypz8rKgqIomuGGG27QTFNSUoKRI0ciOjoaoaGhyMzMxJEjRy5pRYiIiOjqVOmwcvr0aTRp0gTPP/+8z2m6deuG/Px8dVi5cqWmfNSoUVi6dCkWLVqETZs2obi4GDfffDPOnTtX+TUgIiKiq5q5sjNkZGQgIyPD7zQWiwUOh8NrWWFhIV5++WW8/vrr6NSpEwBgwYIFcLlc+Pjjj9G1a9fKVomIiIiuYtXyzMq6desQExODunXrYujQoTh+/Lhatm3bNpSWlqJLly7qOKfTiZSUFGzevNnr8kpKSlBUVKQZiIiI6M+hysNKRkYGFi5ciDVr1mDGjBnYsmULOnbsiJKSEgBAQUEBgoKCEBERoZkvNjYWBQUFXpc5depU2O12dXC5XFVdbSIiIjKoSn8NdCH9+/dX/05JSUGLFi2QkJCAFStWoHfv3j7nExEoiuK1bMKECRg9erT6uqioiIGFiIjoT6Laf7ocFxeHhIQE5OXlAQAcDgfOnj2LkydPaqY7fvw4YmNjvS7DYrEgPDxcMxAREdGfQ7WHlRMnTuDw4cOIi4sDADRv3hyBgYFYvXq1Ok1+fj727NmDtLS06q4OERERXWEq/TVQcXEx9u3bp74+cOAAduzYgcjISERGRiI7Oxt9+vRBXFwcDh48iIkTJyI6Ohq9evUCANjtdgwZMgRjxoxBVFQUIiMjMXbsWKSmpqq/DiIiIiJyq3RY2bp1Kzp06KC+dj9LMnjwYMyZMwe7d+/Ga6+9hl9++QVxcXHo0KEDFi9eDJvNps4za9YsmM1m9OvXD2fOnMFNN92E+fPnIyAgoApWiYiIiK4miojI5a5EZRUVFcFut6OwsBCNp2xUxx/M6X4Za0VERET+eF6/K/P8Kf83EBERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZGsMKERERGRrDChERERkawwoREREZWqXDyoYNG9CjRw84nU4oioJly5apZaWlpXjkkUeQmpqK0NBQOJ1ODBo0CEePHtUsIz09HYqiaIYBAwZc8soQERHR1afSYeX06dNo0qQJnn/+eV3Zr7/+iu3bt+PRRx/F9u3bsWTJEnz77bfIzMzUTTt06FDk5+erwwsvvHBxa0BERERXNXNlZ8jIyEBGRobXMrvdjtWrV2vGPffcc7j++utx6NAh1K5dWx0fEhICh8NRofcsKSlBSUmJ+rqoqKiy1SYiIqIrVLU/s1JYWAhFUVCjRg3N+IULFyI6OhqNGjXC2LFjcerUKZ/LmDp1Kux2uzq4XK5qrjUREREZRaXvrFTGb7/9hvHjx2PgwIEIDw9Xx99+++1ISkqCw+HAnj17MGHCBOzcuVN3V8ZtwoQJGD16tPq6qKiIgYWIiOhPotrCSmlpKQYMGICysjL861//0pQNHTpU/TslJQXJyclo0aIFtm/fjuuuu063LIvFAovFUl1VJSIiIgOrlq+BSktL0a9fPxw4cACrV6/W3FXx5rrrrkNgYCDy8vKqozpERER0BavyOyvuoJKXl4e1a9ciKirqgvPk5uaitLQUcXFxVV0dIiIiusJVOqwUFxdj37596usDBw5gx44diIyMhNPpxK233ort27fj/fffx7lz51BQUAAAiIyMRFBQEL777jssXLgQf/nLXxAdHY29e/dizJgxaNasGdq0aVN1a0ZERERXhUqHla1bt6JDhw7qa/eDr4MHD0Z2djaWL18OAGjatKlmvrVr1yI9PR1BQUH45JNPMHv2bBQXF8PlcqF79+6YPHkyAgICLmFViIiI6GpU6bCSnp4OEfFZ7q8MAFwuF9avX1/ZtyUiIqI/Kf5vICIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjK0SoeVDRs2oEePHnA6nVAUBcuWLdOUiwiys7PhdDphtVqRnp6O3NxczTQlJSUYOXIkoqOjERoaiszMTBw5cuSSVoSIiIiuTpUOK6dPn0aTJk3w/PPPey2fNm0aZs6cieeffx5btmyBw+FA586dcerUKXWaUaNGYenSpVi0aBE2bdqE4uJi3HzzzTh37tzFrwkRERFdlcyVnSEjIwMZGRley0QEzz77LCZNmoTevXsDAF599VXExsbijTfewPDhw1FYWIiXX34Zr7/+Ojp16gQAWLBgAVwuFz7++GN07dr1ElaHiIiIrjZV+szKgQMHUFBQgC5duqjjLBYL2rdvj82bNwMAtm3bhtLSUs00TqcTKSkp6jTllZSUoKioSDMQERHRn0OVhpWCggIAQGxsrGZ8bGysWlZQUICgoCBERET4nKa8qVOnwm63q4PL5arKahMREZGBVcuvgRRF0bwWEd248vxNM2HCBBQWFqrD4cOHq6yuREREZGxVGlYcDgcA6O6QHD9+XL3b4nA4cPbsWZw8edLnNOVZLBaEh4drBiIiIvpzqNKwkpSUBIfDgdWrV6vjzp49i/Xr1yMtLQ0A0Lx5cwQGBmqmyc/Px549e9Rpqkri+BXqQERERFemSv8aqLi4GPv27VNfHzhwADt27EBkZCRq166NUaNGYcqUKUhOTkZycjKmTJmCkJAQDBw4EABgt9sxZMgQjBkzBlFRUYiMjMTYsWORmpqq/jqIiIiIyK3SYWXr1q3o0KGD+nr06NEAgMGDB2P+/PkYN24czpw5g/vvvx8nT55Eq1at8NFHH8Fms6nzzJo1C2azGf369cOZM2dw0003Yf78+QgICKiCVSIiIqKriSIicrkrUVlFRUWw2+0oLCxE4ykb1fEHc7prpvP8+qd8GREREf2xPK/flXn+lP8biIiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDI1hhYiIiAyNYYWIiIgMrcrDSmJiIhRF0Q0jRowAAGRlZenKbrjhhqquBhEREV0lzFW9wC1btuDcuXPq6z179qBz587o27evOq5bt26YN2+e+jooKKiqq0FERERXiSoPKzVr1tS8zsnJwbXXXov27dur4ywWCxwOR1W/NREREV2FqvWZlbNnz2LBggW4++67oSiKOn7dunWIiYlB3bp1MXToUBw/ftzvckpKSlBUVKQZiIiI6M+hWsPKsmXL8MsvvyArK0sdl5GRgYULF2LNmjWYMWMGtmzZgo4dO6KkpMTncqZOnQq73a4OLperOqtNREREBqKIiFTXwrt27YqgoCC89957PqfJz89HQkICFi1ahN69e3udpqSkRBNmioqK4HK5UFhYiMZTNqrjD+Z018yXOH5Fpcs8x3ubj4iIiC5OUVER7HY7CgsLER4eXuH5qvyZFbfvv/8eH3/8MZYsWeJ3uri4OCQkJCAvL8/nNBaLBRaLpaqrSERERFeAavsaaN68eYiJiUH37v7vTJw4cQKHDx9GXFxcdVWFiIiIrmDVElbKysowb948DB48GGbz/9+8KS4uxtixY/Hpp5/i4MGDWLduHXr06IHo6Gj06tWrOqpCREREV7hq+Rro448/xqFDh3D33XdrxgcEBGD37t147bXX8MsvvyAuLg4dOnTA4sWLYbPZqqMqREREdIWrlrDSpUsXeHtu12q1YtWqVdXxlkRERHSV4v8GIiIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkMzX+4KXEkSx6/QvD6Y091rmed4IiIiujS8s0JERESGxrBCREREhsawQkRERIbGsEJERESGxrBCREREhsawQkRERIbGsEJERESGxrBCREREhsawQkRERIZW5WElOzsbiqJoBofDoZaLCLKzs+F0OmG1WpGeno7c3NyqrgYRERFdJarlzkqjRo2Qn5+vDrt371bLpk2bhpkzZ+L555/Hli1b4HA40LlzZ5w6dao6qkJERERXuGr530Bms1lzN8VNRPDss89i0qRJ6N27NwDg1VdfRWxsLN544w0MHz7c6/JKSkpQUlKivi4qKqqOahMREZEBVcudlby8PDidTiQlJWHAgAHYv38/AODAgQMoKChAly5d1GktFgvat2+PzZs3+1ze1KlTYbfb1cHlclVHtYmIiMiAqjystGrVCq+99hpWrVqFF198EQUFBUhLS8OJEydQUFAAAIiNjdXMExsbq5Z5M2HCBBQWFqrD4cOHq7raREREZFBV/jVQRkaG+ndqaipat26Na6+9Fq+++ipuuOEGAICiKJp5REQ3zpPFYoHFYqnqqhIREdEVoNp/uhwaGorU1FTk5eWpz7GUv4ty/Phx3d0WIiIiIuAPCCslJSX46quvEBcXh6SkJDgcDqxevVotP3v2LNavX4+0tLTqrgoRERFdgar8a6CxY8eiR48eqF27No4fP44nn3wSRUVFGDx4MBRFwahRozBlyhQkJycjOTkZU6ZMQUhICAYOHFjVVSEiIqKrQJWHlSNHjuC2227DTz/9hJo1a+KGG27AZ599hoSEBADAuHHjcObMGdx///04efIkWrVqhY8++gg2m62qq0JERERXgSoPK4sWLfJbrigKsrOzkZ2dXdVvbViJ41eofx/M6V7hMiIiIuL/BiIiIiKDq5YebOnSed5xAbR3XfyVERERXW14Z4WIiIgMjXdWrjJ8BoaIiK42vLNCREREhsawQkRERIbGsEJERESGxrBCREREhsYHbP9E+PAtERFdiRhWCACDDBERGRe/BiIiIiJDY1ghIiIiQ2NYISIiIkNjWCEiIiJDY1ghIiIiQ2NYISIiIkPjT5fJL8+fNAP8WTMREf3xeGeFiIiIDI13Vuii+bvr4q+TuYstIyKiPyfeWSEiIiJD450VuiJU9C6OvzLe4SEiujIxrBBVAh84JiL64/FrICIiIjI0hhUiIiIyNIYVIiIiMjQ+s0JURfg8CxFR9eCdFSIiIjI0hhUiIiIyNIYVIiIiMjSGFSIiIjI0hhUiIiIytCoPK1OnTkXLli1hs9kQExODnj174ptvvtFMk5WVBUVRNMMNN9xQ1VUhIiKiq0CVh5X169djxIgR+Oyzz7B69Wr8/vvv6NKlC06fPq2Zrlu3bsjPz1eHlStXVnVViIiI6CpQ5f2sfPjhh5rX8+bNQ0xMDLZt24Z27dqp4y0WCxwOR1W/PREREV1lqv2ZlcLCQgBAZGSkZvy6desQExODunXrYujQoTh+/LjPZZSUlKCoqEgzEBER0Z9DtfZgKyIYPXo0brzxRqSkpKjjMzIy0LdvXyQkJODAgQN49NFH0bFjR2zbtg0Wi0W3nKlTp+Kxxx6rzqoSVSvP3m3L92zrq4w94hIRnVetYeWBBx7Arl27sGnTJs34/v37q3+npKSgRYsWSEhIwIoVK9C7d2/dciZMmIDRo0err4uKiuByuaqv4kRERGQY1RZWRo4cieXLl2PDhg2Ij4/3O21cXBwSEhKQl5fntdxisXi940JERERXvyoPKyKCkSNHYunSpVi3bh2SkpIuOM+JEydw+PBhxMXFVXV1iIiI6ApX5Q/YjhgxAgsWLMAbb7wBm82GgoICFBQU4MyZMwCA4uJijB07Fp9++ikOHjyIdevWoUePHoiOjkavXr2qujpERER0havyOytz5swBAKSnp2vGz5s3D1lZWQgICMDu3bvx2muv4ZdffkFcXBw6dOiAxYsXw2azVXV1iIiI6ApXLV8D+WO1WrFq1aqqflsiIiK6SvF/AxEREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoTGsEBERkaExrBAREZGhMawQERGRoVXrf10mouqROH6F5vXBnO6XqSZERNWPd1aIiIjI0Hhnhegq43nXpfwdl4sp83cXp6JlV1M9iOiPxzsrREREZGi8s0JEVAl/9B0eIuKdFSIiIjI43lkhIjIw3nUhYlghIroiGeWBY/6Mnv4I/BqIiIiIDI1hhYiIiAyNYYWIiIgMjc+sEBFRtTDKszNGrAd/vl45DCtEREQGxoeb+TUQERERGRzDChERERkawwoREREZGsMKERERGRrDChERERkafw1ERER0lbnafinEOytERERkaAwrREREZGj8GoiIiOhP5HL2HrxrYtuLqDHvrBAREZHBMawQERGRoV3WsPKvf/0LSUlJCA4ORvPmzbFx48bLWR0iIiIyoMsWVhYvXoxRo0Zh0qRJ+PLLL9G2bVtkZGTg0KFDl6tKREREZECXLazMnDkTQ4YMwT333IMGDRrg2Wefhcvlwpw5cy5XlYiIiMiALsuvgc6ePYtt27Zh/PjxmvFdunTB5s2bddOXlJSgpKREfV1YWAgAKCoqQlnJr+r4oqIizXwXU+Y5/mLLWA/Woyrq8WdcZ9aD9fiz1OPPuM6eZSKCSpHL4IcffhAA8t///lcz/qmnnpK6devqpp88ebIA4MCBAwcOHDhcBcPhw4crlRsuaz8riqJoXouIbhwATJgwAaNHj1Zfl5WV4eeff0ZUVBQURUFRURFcLhcOHz6M8PBwzby+yi5mnuoou1rfi/VgPVgPY74X68F6XM562Gw2nDp1Ck6nE5VxWcJKdHQ0AgICUFBQoBl//PhxxMbG6qa3WCywWCyacTVq1NBNFx4erttIFyq7mHmqo+xqfS/Wg/VgPYz5XqwH63G56mG3272W+3NZHrANCgpC8+bNsXr1as341atXIy0t7XJUiYiIiAzqsn0NNHr0aNx5551o0aIFWrdujblz5+LQoUO49957L1eViIiIyIAuW1jp378/Tpw4gccffxz5+flISUnBypUrkZCQUOllWSwWTJ48WfdVkb+yi5mnOsqu1vdiPVgP1sOY78V6sB5GqkdFKSKV/f0QERER0R+H/xuIiIiIDI1hhYiIiAyNYYWIiIgMjWGFiIiIDO2qCCt8RpiIiOjqdVm7268qFosFO3fuRIMGDS53VYjoKpSfn485c+Zg06ZNyM/PR0BAAJKSktCzZ09kZWUhICDgcleR6Kp2Rf102fP/A3maPXs27rjjDkRFReHEiRNIT09H69atUb9+fXz99deYPXs2Tp8+jebNm6Nz585o2LChZv7ffvsNb731FgYNGuR1+YcPH8bkyZPxyiuvAACuueYarFq1CpGRkdi1axeaNGmCyMhI/PTTT3j55ZdRUlKCvn37eg1PZ86cwZtvvum10bvpppt8rvuxY8cwbdo0PPbYYwgLC9OUlZaW4tNPP0W7du38br+TJ0/i1VdfRV5eHuLi4jB48GC4XC6/81SVI0eOoEaNGrq6FxUV4amnnsKPP/6o2x6//vortm7dim7duqF169ZYs2YNpk+fjl9//RWZmZnq8bBx40b8+9//xqFDh1CrVi0kJibi2LFjKCgogKIoiI2NRZs2bWC327Fnzx7d8srKytC7d28MGzas2rfDjBkzcOutt3rtT+jLL79EjRo1kJSUBABYsGAB5syZg0OHDiEhIQEPPPAABgwYUK31cx/bycnJPqc5ffo03njjDWzevFm3jW+77TaEhoZWaplVcWxXxnPPPYetW7eie/fu6NevH15//XVMnTpVPQ4ef/xxmM3//zlu69at6NSpE5KSkmC1WvH555/j9ttvx9mzZ7Fq1So0aNAAq1atgs1mq7I6Xg6lpaVYsWIF8vLyYLFYcMstt6jHqec5lpCQgBEjRqB169a6+eLi4tCrVy/dMXAh/o4pX+dtbm4u+vXrh2eeecbrMv21tU2bNq10230x/J3vF5onODgYL7zwAv7+97/7nLYqtr0vR44cQXBwMKKjowH4PwbKO3HiRNVv34v8x8mXhaIo0rRpU0lPT9cMiqJIy5YtJTU1VRRFkcjISAkODpYPPvhAatasKa1bt5bg4GABICaTSdq3by9Hjx5Vl1tQUCAmk0n3frNnz5bZs2fLuHHjRFEU9XVAQIAMHjxYrFarKIoiERERsnXrVklKSpLk5GSJiooSi8Ui27Zt0ywvLy9PEhISJCoqSuLi4kRRFOnevbu0atVKAgICpG/fvlJaWqqrx9GjR6VRo0YCQAICAmTQoEFy6tQptfy7774TRVEkKytLunXrJhkZGZKVlSV2u12+//57ERHZv3+/OBwOcTgc0rlzZ4mPjxe73S5fffWVupykpCT59ttvpbi4WObOnatb3osvvijFxcU+909BQYE89thjmnEul0tSU1PFZDLp6p6Xlyfx8fECQLc9FEURRVHkuuuuk/DwcFmwYIHYbDa55557JDY2VoKCguTZZ5+VZcuWiclkkszMTBkyZIi6n1u1aiXDhg2ToUOHyi233CJWq1UASKNGjXTLGz58uAQHB8uAAQPkrrvukm7dukn37t3lgQcekI8//lhERA4fPqzZ5m5nz56V9evXq38vXbpUpk2bJq+//rocO3ZMtx3dx2CHDh1k0aJFUlJSoi6rWbNmsmbNGhERefHFF8VqtcqDDz4oc+bMkVGjRklYWJi8/PLLujq4XC757LPP1NcbNmyQgQMHyo033ii33367TJs2Tf7+97/L5s2bRUTkk08+kQYNGkj9+vWlf//+6nHtPrYnTJggs2fPlmeeeUZefvllzTYZOHCgREVFSY0aNeSWW27RbWO73a7O722Zs2fPrtSx7evcdNu+fbtMnDhRN/7kyZMyduxY3TF8yy23iM1mkz59+ojD4ZCcnByJioqSJ598UqZMmSI1a9aUUaNGaerQpk0byc7OVvf166+/Lq1atRIRkZ9//llSUlKkV69ecuLECRER+fHHHyUnJ0dGjx6t+c/y5feLe394O3Y8zzPPsr/+9a8yfvx4zb7MyMiQrl27yowZM2TNmjW6ejz22GOyd+9e3TZq3bq1nDx5UgoKCmTcuHGSmpoqQUFBkpycLIqiSM2aNeXIkSOac+yRRx6RyMhIMZvN8t5778nx48c18wUHB0t8fLxMnz5ddy4NHz5cDh48qKtHbm6uOJ1On8eUr/MW//vvvTVr1pScnBzJz89Xl+mvrTWZTBIYGOi17a5Tp45YrVZZuXLlBc93T3v37pWsrCzdtlcURQICAqRTp066890X9zzudtBzHvc+ExGv297pdMquXbvU6d3HXGJiotxyyy2aY87TTz/9pDt2ateuLXfccYfs3btXdwz06tVLAgICZMCAAbpjsVWrVj6vje7tW/7aWBFXVFiZMmWKJCUlySeffKIZbzabJTc3V1q3bi2TJk0SEZE333xTIiIiZOLEidKzZ0+5+eabZdSoUdKmTRvp0aOHJCUlqRfyV199VRRFkXfffVczKIoiUVFREhkZKQAkMTFREhMTRVEUsVgsEhYWJgkJCfLMM89IfHy83HPPPSIi6oW2Zs2amgM0IyNDhg8fLufOnRMRkalTp0pGRoaIiCxfvlycTqfce++9snPnTs3Qo0cPtQFZvXq1tGjRQpo3by4///yz5ObmisPhEAC6Ex2AOBwOyc3NlQEDBkh6erqcPn1aZs+eLTNmzJBGjRpJkyZNNBeVoUOHit1uF6vVqltejRo1pFatWpKbm6vbN95C3ezZs0VRFHE6nTJmzBi5//77NXXPyMiQQYMGiTsze26Pa6+9VqKiomTy5MmyZs0aCQ4Oln/+858iImKz2eSZZ56RBg0aSKtWrSQnJ0dERNLT02XAgAEya9YsadasmaZ+9evXl5YtW0p6erpueXl5eRIVFSUBAQFeG7aIiAivYat169byzTffiMlk0jUcQUFBEhAQIHa7XbMdAYjVapXg4GAxm80SFRUlDz30kOzevVtCQkLUY7JZs2bywgsvaNbhzjvvlNjYWM32nT17tgCQfv36yezZs+Wee+7RNCpNmzYVAFKnTh1NYw9AQkND1XDveWzXqlVLatWqpdbPc5vYbDZRFEX69OmjC9aKokhISIgEBwery0tMTBQAEhMTI06nU2rVqlXhY1vkfFhRFEV3vB09elRatmwpJpNJAGj2i79zwmQySWRkpOTm5sqOHTskICBAFixYoC6zTp06uuBktVrlu+++U4PTuXPnJDAwUAoKCuTzzz+X0NBQAaBrmIODgyUoKEi2bdtWqQu+oigSHx8vR44c0ZTVrFlTAEhQUJDYbDbNhbtXr14CoFIXCEVR5NixY7Jjxw4BIE2bNlUv+GFhYXLdddfJ3XffrTnH3PNNmTJFmjVrJkOHDtXM98UXX4jFYhGLxaI7l9z1u+mmmzTtovu89XYh93feKooiDz/8sEREREh0dLQEBgZKZmamvPfee9KtWzefbW1aWpqEhYXJ+PHjdW330aNHJTo6ulLh+fPPP5ewsDCvxwAACQwMlPbt20tgYKDmfC/fzrsHRVHk8ccflxYtWggAzTzufSYium3/008/ic1mk86dO4uIaI4597ZXFEXGjRun2daff/652O123bFjMpkkISFBrFarpKSkaI6BOXPmiMlkEqvVqguRtWrVkoCAAMnJydFtXxGRIUOGSM+ePXX7+kKuqLAicv5kqFu3rowZM0bOnj0rIv8fVsLDwyUvL09ERM6dOydms1m2bdsmMTExsmvXLtm9e7fExsaKiMj9998vtWvXVu9KeO5M9+BO7u5Pw25ms1nCw8PVTytnz54Vk8kkn3/+uYicP4mys7MlODhYc4BarVb59ttv1eWUlJRIYGCg/PTTT6Ioitrw+qqHuw6//fab3HLLLdK0aVO58cYbpWfPnl4bdEVRpFevXpKenq4Jee7GMC4uTgICAjQXqqCgIAkNDZXExETd8rZu3SrdunWTFi1aeD3BoqKiNKHOfaGqWbOmJCYmSlJSkqbuISEhsnnzZnW9PLeH1WqVF198Ua1HYGCg7N69W0RE7Ha7rFy5UkJCQiQmJkZ27twpIiJWq1Vyc3Nl3759EhISoqm71WqVjz76SKxWq255GRkZMnDgQLXMs2Hr2bOnWCwWGTp0qO5iqiiK2oCUbzjatGkj0dHRMnjwYN0+OXz4sNx2222SlpYmTz/9tNSvX19MJpOYzWaZNGmSFBUVSUxMjOzYsUM3r6Iomu3r3saxsbGSmJgoFotF06g0aNBABgwYoN61cTf2w4YNk6ZNm8pTTz0lDRo00Bzbubm5PoO1+1NnYmKiTJ48WVO/YcOGSf369cVisWjGu4/d8sf1hY7tEydOyJo1a0RRFK8BvnHjxjJlyhRRFEWzX9LT06Vnz55qCC5/HGRmZkp6erp6HOzZs0dERAYNGiTNmjUTi8Wi2dcul0s2bdqkBqejR4+Koijy66+/SqdOnaR///5isVh0DbPNZpN+/fpJz549K3XBd6/P3XffrSlr0KCBzJw5U9LS0iQjI0Nz4e7UqZO0a9dO6tatq6vHzp07pVevXtKhQwfdObtmzRpZvHixAJD3339frZ/dbpeXXnpJEhMTNeeYu36ff/65hISESN26dTXzZWRkSI8ePSQhIUFz3Ljni4qKknr16mnaxeDgYK8fgNz7y9d5qyiKbNmyRUJCQuTs2bOyePFi6dq1qwQEBIiiKHLvvfeq1wPPtiUiIkKee+45SUxM1LXdgwYNktTUVKlZs6bmGNi4caPs3LlT1qxZIwA02/GGG26Qjh07iqIoXj+4Dhw4UHr27CnHjh3TnO/e2npv16Ly80yfPl2Kiop0215EJCQkRGrVqiUiojnmFEWRefPmqd8+eAagTp06yT333CNFRUWa+tvtdtm5c6cMGTJELBaL5hho0KCBPPnkkxISEqILkREREWq7Un77ipy/G+quY2VccWFFROTUqVMyaNAgady4sezatUsCAwN1YUXk/KeD7777Tmw2m+zdu1cOHjwowcHBavkDDzwg8fHxEh0d7fViLyKydOlScTgcmnKz2SxWq1UOHDigey+R8wfGtm3bJDg4WHOAApCGDRvK3LlzpaioSE6ePCmKokhRUZFER0dLTk6OBAUFycGDBzVDSEiIzJ07VxOYSktL1ZCydOlSr7fKFUWR9evXi9VqFafTqTbK7gvV6tWrNRcWs9nst+GoSKAqvx0ByMqVKzXj3HU3m82yaNEite6e2yM+Pl4WLVokFotFfvjhB1EURVasWCEiIpmZmTJw4ECJj4+Xrl27ql8tOJ1OWbZsmbz44ouSnJysec/4+Hh56qmnxOl06pYXEhIiCxYskPj4eBHRNmxOp1OmTZumhibPi6k7rJhMJl3DYbVaZd68ebrQ5/5ktHv3brUBFjl/u/aaa64Rs9ksoaGh0rdvX/nb3/6mmbdly5YSHBysu6UPQJYsWSIioruwWK1W2bBhgxrePBv7pUuXitPplMDAQHV6d1gJCQnxGqwdDocsW7ZMli1b5jXQjh8/XgICAuS5557T1O+JJ57QHdcXOrYbN27s94OE+2+TyaTZL1arVdavX+/1nEhKSpI5c+aoHxxMJpO89dZbInL++Jk5c6ZuX0dHR0uDBg3kjTfeEEVRpEOHDmrYiYiIkLlz58q1116ra5jtdrssWrRIatWqVakLvqIosmTJEklMTNSUWa1W+f7772Xt2rWSmJio2ZcRERGyevVq9cJd/sOTt/PW8zUAzXmfmZkp9913n1gsFs055l7ezJkzJTk5WWJiYjTzhYSEyNq1a9V2pfwHsvnz50tiYqKuXaxTp47aLnryd9662z73eev2/fffi81mE4fD4bVtCQ0NlQ0bNqh19Gy7nU6nvPvuu+p1wn0MeAZub9vRXe5t23/88ce6i7P7/YOCgsRqtWrOCUVRZOvWrbJixQrNMbxhwwYBICEhIRIaGqrb9iLnA7L7fPY85tztzr59+8RqtWoCUEBAgDz22GNSVFSkqX9mZqaMHz9etm/fLhaLRXMMWK1WycnJUdtZz2MxNDRUNm7cqLY5ntvXvX88r8MVdUWGFbc333xTYmNjxWQySW5urjRu3Fg++OADtXz37t1SWloqLVu2lNdee002btwoSUlJmmWMGDFCzGaz109hbqtWrRIA0q1bN8nPzxez2SzXXHON5uuo999/X3799VcROX9grFy5UncSdevWTWJjY8VqtYrVapX+/furX1d07dpV7r77bnG5XLr3T01NlenTp+uCQGlpqQQHB0t0dLTPsJKQkCCBgYESFhamXtBEzl+oYmJixG63q+PMZrPExMTIsmXLvG6H6OhoGTFihMTExHi98Pztb38TALoL1axZs3TLKi0tFZfLJRaLRRRFkf3792u2x4gRI9Tnaq6//noZPHiw1K9fXz744AOZO3euBAQESJ06deSJJ56QsLAwueOOO6Rjx44SGBionnz5+flSUFAgO3bskLS0NDGZTNKxY0fd8qKioqROnTpy9913i4i+Yfvkk080oc59MQUg69atE5PJpGs4nE6nzJ07V3eXwWQyybFjx9Sg4OmHH36Q2rVrS3JysowePVqsVqvceOONMnToUGnXrp0EBQXJ3/72N3G5XJptrCiK+kmu/IUlPj5eHn74YUlOTtY19iIib7/9tlgsFs2x7X6GwPNrA/c2GT9+vPpcSmBgoGYbP/PMMxIRESFjxoyRjh07qstUFEVGjhzp9Zjyd2z37NlTvUB4Czlr167VNOieIXj27Nlez4lJkyZJeHi4hISESFJSkkyYMEFq164tc+bMEYvFIg6HQ/76179q6nHzzTeL3W5X24m0tDTZv3+/iJxvmF999VU18Hg2zJmZmXL//fdLcHBwpS747g87FotFUxYfHy8bNmyQgwcPSlBQkGZfhoaGyptvvqm2OZ71iI6OlmnTponFYtFdFDt06CCtW7fWfajYu3ev2O12CQ4O1pxjTz31lHphvu666yQiIkIzn9PplPnz56t3sD3PJUVR1K+JPGVlZUlgYKAEBQVJcHBwhc9bRVGkQYMG6nnrafDgwdK+fXt56aWXdG1L/fr1ZebMmWpb69l2h4aGyltvvaVpu0tLSyUoKEji4+Plww8/1B2PISEh8sorr6jHm+e2N5lM6gfX8rp27Sp/+9vfZO7cuZrx7jZix44duvNCURTp0qWLNGvWTLftRUTatm2rhgTPY84dVsp/kNuwYYP64Ts0NFRT/71790pUVJT06dNHzGaz5hgICQmRwMBAmTdvnq5dcW9f9zb03L4iIp999pnu2lgRV3RYETn/4OOyZcukuLhY5syZo7stJnL+WZeMjAyZOHGiDBkyRFfuTs6+FBcXy9q1a2XKlCnicDgkICBARowYIW+++abX6U0mkzz00EPSu3dvzfhjx47JDTfcoLmdv337dhERWbJkiYwcOVL+8Y9/6JY3btw46dChg8yfP19X9uijj4rZbBZFUWTHjh2aE71z584SHBws6enpkp2dLR9++KFm3uHDh0tMTIzmQnX//feL3W6XZ555Rre8unXrSnBwsO4hWjf3d9/lL1RpaWlep//hhx8kIiJCbfw8t0dxcbHcdNNNEhcXJ/fee6+cPXtWnnnmGbWRbtWqlfTs2VN9hkJRFAkMDJTatWtLRESE+mnSfbGLjY2Vli1bSkpKim55AMRut8umTZt0DVtqaqo89thjuhBZWloqANQHess3HJMnT5awsDAJCwvTbEdFUWTy5MkSERHhdTuePHlSHnnkEWnYsKH6zENCQoIMHDhQtmzZIiIiR44c0WzjgIAAqVGjhgwaNEh3YalXr54oiiK9e/fWNfYffvihpKamyl133aU5tnNzc9XG/quvvtJtE/dDqe795t7GcXFx8vTTT4uISFlZmbpMk8mk+QrEk79ju7S0VGrWrOn1rmdqaqq88847uga9tLRUXWdv58TTTz8twcHBUrduXbVOb775prhcLgkICJD09HTdQ+TuEORyuXR1qV+/vs8PLe4LfkhISKUu+O5zxmKxaMpGjBghycnJMnz4cDGbzZp96XK5JCkpSb1we9aja9euMmzYMN0FIisrS7KystRnGtyBy+2ee+4Rh8OhO8dq1qwpHTt2VOf3nG/w4MESHx8vbdq00R03JpNJli5d6vUDWU5OjnoHuzLnbZs2bdRnODx5trXl25bs7GwZNWqU17Y2NTVVevfurWu7u3TpIg0aNJDatWvrQnD9+vVl7ty56rFRkQ+uIufb/Ndff1033h0sfv75Z9154d7m3ra9iMhdd90lgYGBurZAURTp27evWCwWmTdvnq7+y5cvV0OTZ/337dsnnTt31txJct9hjYuLkyeffFLXrtxxxx3icrm8hkgRkYkTJ+q2b0Vc8WHlj7Z161Z59tln1QcAvVEURQ4cOCC//fab1/Jvv/1WvetTEaWlpVJYWOizfMqUKRITE6M70T0vHr54XlTcF6qcnBz14TjP5UVEREj//v19Lst9cpW/UHl+X1ne77//LmvXrtVsj7KyMp/Tnz59WnOruKysTAoKCuTo0aOah8b2798vmzdvls2bN2tuQZZ35swZ+e6773QNm/uuwrhx46RJkyZeG7ZBgwaJy+USAF4bjrZt26p3ji60X/ytszfl99sHH3wg/fv3111YWrVqJZ06dfLa2CuKIunp6Wpjv3XrVpk5c6b8/PPPfht7kfN3ZCZPnqxuY/edhvL8nS9lZWU+j2339njnnXe83pkbN26cdOnSxWuDXlpaKg0aNFBv11f0nHAv05vS0lLJzMzUhZXs7GyfH1pERO677z6Jj4+v1AU/KytLGjVqJE6nU1NWXFws99xzj0RHR4vL5dJduBs0aOD1wr1kyRLJzMz0eYHwtg3d73fmzBnNOeZ+TtCbsrIyOXbsmFx//fVejxtFUeSll17yei65eZ63vo4pkfPnbfmvjLypbFs7btw4uemmm3Rt95IlS2T+/PmSmZmpCyvZ2dny4osvet2GIhd/cb4YxcXF6g8qPI85ANKiRQtZunSpbp4LHcMTJ06UXr16aY4B97FYkXbFzX1Onz592ue10Z8rqp8V8u/AgQMoKCgAADgcDrW/jorYtm0bNm3ahEGDBiEiIuKSlwcA27dvx8aNGzXLrIigoCCfnfxdTJm/eTzl5eWhpKQE9evXV/vZ+P333/Hrr78iPDzc6zznzp3DkSNHvPajcPr0aQQEBCA/P/+C27GidSyv/H4TERw/fhxlZWWIjo5GYGCg1/l+++03lJaW6voGKV8Pb9ukqlzsfgYqvl/KysoqfAxfyr725ddff0VAQACCgoIqtF/c3MdOcHDwBct87Utv9bBYLBWue2V57rPqPG6qU3UeA9W57b2paFtwIZWpv79j8WLbOLcr5yi6TM6cOYNt27YhMjJS15ncyZMnMWPGDAwcONBvR3Puztg2b94Ml8uFUaNGweVy6Tr9GjZsGK655ppKvdfIkSNxyy234OjRoxg0aFCFA8VXX32Fzz77TO08LzQ0FF9//TXGjBmDO+64Ax07dkRSUhJq1KihdiTndDoxaNAgtSM5f53MnTx5Ehs2bMDmzZtx6NAhr+tcVlaGpk2bol69epq6nTt3Dn369IHVagUAtG/fvkJlpaWlXsvOnTuHhx9+GLGxsbDb7Zg5c6bfDtfKr1daWhq+//57XUeDJSUlaNu2LQAgLS0N9erV05R5bkf3fnF3ZHfq1Cl8+umn6NKli1rHnJwcREVFAQBmzpxZof3YvHlzNG/eHF999RVeeeUVbN68Gf/5z3/w9ddf4/HHH9fUw9PEiRMvqh7ujuvMZjMmT56Mjh07qtsxKioKd999Nx588EEA/9+p3a5duxAeHq7b1xfaz/7qYTabER4ernbYOGPGDM0+a9OmDQ4ePIi0tDS0bt0aX3/9NaZNm+Zze3gu05ejR4/iscceUzuHrIgTJ06oHUoGBQVd1Lnk6auvvsLKlSs1+3n27NlYv349hg0bhlGjRnmtx/fff68537/++ms8+uijAID77rsPHTt2rHB7NHr0aPz+++/49ttvNePd+wwACgoK8I9//ANms9nr+VKZTjsr2s6W56vt/vLLL2G1WvHFF19g0KBBmvU+d+4chg0bhuzsbM2yRo4ciX79+qFt27Zeg0r59tRznfv27QubzVbheuzbtw8ulwtjx47FgAEDKnWd8Nweyv861vNUvoNTb+Mrer3y1Wbu2LEDDRs21AWWi23jVJW+F/Mn8s0330hCQoJ6S9OzM7lvvvlG7dCsfFlcXJzs3btXTCaTpjM2m80m0dHRYrfb5fHHH9d0+jV48GDNrxsq+l6eT/qX7xjJlw8++ECCgoJ0ned16tRJ7R/kk08+kf3790tcXJzakZzJZBKbzaY+x+DZyZyvMn/r7F6fevXq6Tr5AyBhYWFSo0aNCpfhf0/K16hRQ1OmKOf7/2jSpIl06NBB1+FaaGiohIaGyssvv6xbL3efC+6HDT23lbsPE5vNpiu76aabxGw26/oEUpTzHRu6+1Ao37Fhenq6dOjQoVLHqXt/2u12AVCt9XD/BNr9rIjndqxZs6YEBwfLyy+/rNnGinK+z6GAgADNvr7QfvZXj7i4OPnpp5/UelR0n/naHhWxY8cOvx3Ueauj+wHsip4vvjpsFPG/n/G/X6TUqlVL1w74Ot9tNps0a9ZMzGaz+kB3RdojRVEkJSVFc9y491ndunVFURT1l4UVOV/8ddrpr+0T8d1poL+2u1mzZvLOO++IyWTStQXu7RgbG6vZju7lJCcnV3j7durUSX14uTL1cLlc0qdPHwkLC1M7H63IfvG3Pdy8HcO+jlN/bbe/NtO9r66//vpKndMXwrDih7szuR9//FHy8vI0ncn17NlTffCofJmi/P9PWj07Y3P/HPTmm2+WGjVqaDr96tmzpzRr1kzq1q1b6fd6++23BYCuYyR3Hxnl+eo8T+T8SfnQQw9J586dNXV3l3Xu3FluvfXWCpf5W2d3j6HuPhnczGazjBo1ymcHgL7KTCaTxMfHe53H/bNPEX2Ha4qiyL/+9S9p2LChbr1uuOEGqVOnjtx66626bdW6dWtJS0uTzp0768reffddufXWW6VJkyaajgbdHbvdf//9ur57fP1k3K18p4XuoV69etK3b1+ZNWuW+mzRpdbD13tZLBZ56aWXZNasWQJAsx1DQkJk9uzZ0rBhQ802dnfmOHHiRGnYsGGF97O/eiiKIq+++qpaj4ruM5Hz38G7O86qyPZ1D7NmzdI19P6mV5TznXuVbwfcx5yvc+mdd96RFi1aSFpaWoX3s6Ioctttt0nt2rV17YCv893dCeHEiRPFZrNVuD2aMmWK1K5dW/f8jtlslqZNm/psW3ydL/467fTX9on4vjj7a7tDQkJk69atYjKZvLYFEyZMkMjISM12VBRFPvroI3nooYcqvH3d9ahTp460b9++wvVw75eFCxdKcHBwhfeLiO8OTidOnKj+wKR8ua/j1F/b7a/NfPzxxyUkJETatWunOz4u1Mb5w7Dih7szOU/uzuSioqJk7dq1mhPFXeYZVjwb4qioKNm6dat89tlnYjKZNJ1+xcTEyIoVKzR9b1T2vcp3jOR0OmXixImavmdExGfneSLnD8J169ZJbGys7iLi+WR7Rcv8rbPI+QfXFEXx2smfvw4AfZUtWrTI6/gaNWrI1q1b1W3tWQ9FOd/nhdVq1a1XeHi4vP322xIfH6/bVuHh4bJixQqJjY31uh0v1CcNAN16+eNepq/luV9XRT0q8l4ANNsxKipKli1bJlarVbeNv/jiC0lKShKz2Vyp/VzRelR0n4mIpnPIimxfz6H8hdHfPO46lm8H3PP5O5d89Yvibz+7z1t/HaR5bg/3ueluPyraHn333XfywQcfCADdPgsLC/PZtvg6X/x12umv7fPsVbg8f213RESErFq1Su1ywFdb4Lkd3XdbJk6cKHv37q3Q9nXX4z//+Y/meLtQPdz7Zd++fbpz7EL7xX2c+DvXfR1Xlble+WszRUReeeUVMZvNXs/pi2Wq3JdGfy5nzpzRPRz2z3/+E5mZmfj5559x+PBhr2UiggMHDgAASkpK1O8NMzIyMGfOHPX1O++8o3mv1atXo06dOpV+r/379wMAAgMD0a9fP3z44YfYv38/hg4dioULF+qeCfFkMpkQHByMGjVqqONsNhsKCws1dXeLiYnBjz/+WOEyf+sMAF9//TUaNGiAH3/8ES1atMDu3buhKAoAoGXLlti2bVulylJTU72Ob9u2LebMmQPg/LMR5evx7rvvok6dOl7XKzo6Gj/++KPXbRUaGorCwkJdWVxcHObMmYPg4GCUlZVpBhHBf//7XwDQrZc/cXFx+M9//qNbXnh4OPLy8rB9+3YoilIl9fD1XnfccQeGDBmC7du36/ZnRkYGnnzySdSpU0e3jVu2bIlBgwYhLCysUvvZVz0URcHx48fVelRmn7mP74puX/fgfq+KzqMoCj755BMAqNS55N5nFoulwvsZAMLCwlBYWKhrB4KCgrB8+XLUq1dPsz3c56b72YKKtkft27dHaGgoFEXxus/cKnq+uNtZz/1Skbavffv2+Pbbb3X7xF1/X213SUkJ/vGPfwDw3xZ4bkdFUXDnnXdi4cKFSElJqdD2ddfDbrdrjrcL1cO9X9566y3Y7fZK7ZeoqCgoiqI7Fp1OJ5YuXYovv/wSJpOpQsfphdpuf21mhw4dYDKZ/B4flXbRMedPwN2ZnDcxMTESEhLiNdXjfykV//s+3t0Z2w8//CCJiYnSpEkTCQsL03T6ZbPZJCAgQNNhV0XfKzw83Of3lGVlZfLRRx9pxvnqPE/kfGK+5pprJCgoSNeRnLvMWydzvsr8rbO7ozP3Opfv5M/TxZR5jnf3+tmuXTtdh2v43yeNa6+9VrdejRs3lmnTpqk9UHpuq8aNG8v06dPVjgY9y3r06CGDBw/WdULo5n7Wwt96ldejRw959NFHdePd+9O9zKqoh6/3cu/P5s2bCwDNdnT//5eUlBSfndqtWLGiUvvSVz0URZHU1FS1B9SK7jMR8do5pL/tW35bVXQeRVHU/w9TmXOpR48eMmjQIF2vp/72s6Iosnz5cq/r1bhxY1m5cqXaDrjn8zw3w8PDK9wejRgxQmrUqKG2OZ77rG7duj7bFl/ni79OO/21feXr4clf252VlaW2z97agsDAQN16K8r5vk98tafetq+7HpMmTfK6X3zVY+DAgWqXB7169arUfvHVwan7OPXV0Zy349Rf2+2vzRQRWb9+vXr8VqaN84dhxQ93Z3K+yrx9byty/nfr7gcXy3fGdvLkSbn++uslPDxc0+lXkyZNfHae5u+9EhMT5a677vJa5ouvzvPcdW/btq00a9ZMV/fs7GxJS0uTlJSUSpX5WmfPjs7cPDv5K+9iyjzH++pwLTU1VYYNGybZ2dm6us+ZM0d69eolAwYM8Lod+/Xr57WjwQ0bNkj//v29lomc7w9h3bp1F1yv8sv0vBB41uP999/XLPNS6+HrvUTO78/Ro0dLYmKibn+uWbPmgp3aXWidK1IP976aOHGiZGVlVXifiYjPziH9rXP5bVWReTzrV5nzZcOGDdKnTx9d/f3t58TERBk1apTX9fJ3vp88eVJat24tNWrUqHB7JHK+/xjPNse9z5599lmf7+XrfPHXaae/ts9bPcov05e7775b/fcnnusdGhoqq1ev1k2fmJgoP/30k8/18rXOU6ZMkWuvvdbn+eerHrfeeqsMGjRIN/5C+8VXB6fu49TbsePvOPXVdvtrM0VExo4dqzl+K9rG+cN+VoiIiMjQ+MwKERERGRrDChERERkawwoREREZGsMKERERGRrDChH5lZ6e7vN/zpS3bt06KIqCX3755ZLeMzExEc8++2yFplUUBcuWLfNZfvDgQSiKgh07dlxSnYjo8uE/MiSiK1p+fn6l/qs3EV15GFaI6IrmcDgudxWIqJrxayAiqrAFCxagRYsWsNlscDgcGDhwII4fP66b7r///S+aNGmC4OBgtGrVCrt379aUb968Ge3atYPVaoXL5cKDDz6I06dPX1Sdyn8N9MUXX6BZs2YIDg5GixYt8OWXX2qmf/zxx+F0OnHixAl1XGZmJtq1a4eysrKLqgMRVS+GFSKqsLNnz+KJJ57Azp07sWzZMhw4cABZWVm66R5++GFMnz4dW7ZsQUxMDDIzM1FaWgoA2L17N7p27YrevXtj165dWLx4MTZt2oQHHnjgkut3+vRp3HzzzahXrx62bduG7OxsjB07VjPNpEmTkJiYiHvuuQcA8O9//xsbNmzA66+/DpOJTSKRIV1037dE9KfQvn17eeihh7yWffHFFwJATp06JSIia9euFQCyaNEidZoTJ06I1WqVxYsXi4jInXfeKcOGDdMsZ+PGjWIymeTMmTMiIpKQkCCzZs2qUP0AyNKlS0VE5IUXXpDIyEj139WLnO8OHYB8+eWX6rjvvvtObDabPPLIIxISEiILFiyo0HsR0eXBjxFEVGFffvklbrnlFiQkJMBmsyE9PR0AcOjQIc10rVu3Vv+OjIxEvXr18NVXXwEAtm3bhvnz5yMsLEwdunbtirKyMvW/lV+sr776Ck2aNEFISIjXurhdc801mD59Op5++mn06NEDt99++yW9LxFVLz5gS0QVcvr0aXTp0gVdunTBggULULNmTRw6dAhdu3bF2bNnLzi/+1/El5WVYfjw4XjwwQd109SuXfuS6iiV+FdnGzZsQEBAAA4ePIjff/8dZjObQyKj4p0VIqqQr7/+Gj/99BNycnLQtm1b1K9f3+vDtQDw2WefqX+fPHkS3377LerXrw8AuO6665Cbm4s6derohqCgoEuqY8OGDbFz506cOXPGa13cFi9ejCVLlmDdunU4fPgwnnjiiUt6XyKqXgwrRFQhtWvXRlBQEJ577jns378fy5cv93mRf/zxx/HJJ59gz549yMrKQnR0NHr27AkAeOSRR/Dpp59ixIgR2LFjB/Ly8rB8+XKMHDnykus4cOBAmEwmDBkyBHv37sXKlSsxffp0zTRHjhzBfffdh6effho33ngj5s+fj6lTp3oNNURkDAwrRFQhNWvWxPz58/H222+jYcOGyMnJ0QUBt5ycHDz00ENo3rw58vPzsXz5cvWuSePGjbF+/Xrk5eWhbdu2aNasGR599FHExcVdch3DwsLw3nvvYe/evWjWrBkmTZqEp59+Wi0XEWRlZeH6669Xf33UuXNnPPDAA7jjjjtQXFx8yXUgoqqnSGW+5CUiIiL6g/HOChERERkawwoRGdbChQs1P3H2HBo1anS5q0dEfxB+DUREhnXq1CkcO3bMa1lgYCASEhL+4BoR0eXAsEJERESGxq+BiIiIyNAYVoiIiMjQGFaIiIjI0BhWiIiIyNAYVoiIiMjQGFaIiIjI0BhWiIiIyND+D9rkcpQaggPTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label_idx\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8437147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name\n",
       "기넥신에프정(은행엽엑스)(수출용)    206\n",
       "일양하이트린정 2mg            98\n",
       "뮤테란캡슐 100mg            74\n",
       "보령부스파정 5mg             74\n",
       "동아가바펜틴정 800mg          58\n",
       "                     ... \n",
       "자이프렉사정 2.5mg            4\n",
       "졸로푸트정 100mg             4\n",
       "브린텔릭스정 20mg             4\n",
       "렉사프로정 15mg              4\n",
       "쿠에타핀정 25mg              4\n",
       "Name: count, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"class_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8f2cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['쎄로켈정 100mg', '무코스타정(레바미피드)(비매품)', '펠루비정(펠루비프로펜)', '동아가바펜틴정 800mg',\n",
       "       '트라젠타정(리나글립틴)', '기넥신에프정(은행엽엑스)(수출용)', '트윈스타정 40/5mg', '리바로정 4mg',\n",
       "       '자누메트엑스알서방정 100/1000mg', '플라빅스정 75mg', '리렉스펜정 300mg/PTP',\n",
       "       '큐시드정 31.5mg/PTP', '비타비백정 100mg/병', '아모잘탄정 5/100mg', '자누비아정 50mg',\n",
       "       '리피토정 20mg', '아토젯정 10/40mg', '가바토파정 100mg', '레일라정', '세비카정 10/40mg',\n",
       "       '로수젯정10/5밀리그램', '크레스토정 20mg', '종근당글리아티린연질캡슐(콜린알포세레이트)\\xa0',\n",
       "       '일양하이트린정 2mg', '타이레놀정500mg', '뮤테란캡슐 100mg', '트루비타정 60mg/병', '알드린정',\n",
       "       '에빅사정(메만틴염산염)(비매품)', '아토르바정 10mg', '카나브정 60mg', '자누메트정 50/850mg',\n",
       "       '노바스크정 5mg', '트라젠타듀오정 2.5/850mg', '제미메트서방정 50/1000mg',\n",
       "       '비모보정 500/20mg', '라비에트정 20mg', '보령부스파정 5mg', '다보타민큐정 10mg/병',\n",
       "       '써스펜8시간이알서방정 650mg', '리피로우정 20mg', '글리아타민연질캡슐', '카발린캡슐 25mg',\n",
       "       '엑스포지정 5/160mg', '케이캡정 50mg', '놀텍정 10mg',\n",
       "       '타이레놀이알서방정(아세트아미노펜)(수출용)', '조인스정 200mg', '자이프렉사정 2.5mg',\n",
       "       '콜리네이트연질캡슐 400mg', '울트라셋이알서방정', '에스원엠프정 20mg', '메가파워정 90mg/병',\n",
       "       '맥시부펜이알정 300mg', '삼남건조수산화알루미늄겔정', '신바로정', '란스톤엘에프디티정 30mg', '마도파정',\n",
       "       '리리카캡슐 150mg', '졸로푸트정 100mg', '에어탈정(아세클로페낙)', '아질렉트정(라사길린메실산염)',\n",
       "       '오마코연질캡슐(오메가-3-산에틸에스테르90)', '뉴로메드정(옥시라세탐)', '로수바미브정 10/20mg',\n",
       "       '아빌리파이정 10mg', '삐콤씨에프정 618.6mg/병', '글리틴정(콜린알포세레이트)',\n",
       "       '낙소졸정 500/20mg', '브린텔릭스정 20mg', '스토가정 10mg', '렉사프로정 15mg',\n",
       "       '쿠에타핀정 25mg'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_tmp = train_df[\"class_name\"].unique()\n",
    "classes_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02840906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 클래스 개수 (배경 포함): 74\n",
      "클래스 리스트: ['background', '보령부스파정 5mg', '뮤테란캡슐 100mg', '일양하이트린정 2mg', '기넥신에프정', '무코스타정', '알드린정', '뉴로메드정', '타이레놀정500mg', '에어탈정', '삼남건조수산화알루미늄겔정', '타이레놀이알서방정', '삐콤씨에프정 618.6mg/병', '조인스정 200mg', '쎄로켈정 100mg', '리렉스펜정 300mg/PTP', '아빌리파이정 10mg', '자이프렉사정 2.5mg', '다보타민큐정 10mg/병', '써스펜8시간이알서방정 650mg', '에빅사정', '리피토정 20mg', '크레스토정 20mg', '가바토파정 100mg', '동아가바펜틴정 800mg', '오마코연질캡슐', '란스톤엘에프디티정 30mg', '리리카캡슐 150mg', '종근당글리아티린연질캡슐', '콜리네이트연질캡슐 400mg', '트루비타정 60mg/병', '스토가정 10mg', '노바스크정 5mg', '마도파정', '플라빅스정 75mg', '엑스포지정 5/160mg', '펠루비정', '아토르바정 10mg', '라비에트정 20mg', '리피로우정 20mg', '자누비아정 50mg', '맥시부펜이알정 300mg', '메가파워정 90mg/병', '쿠에타핀정 25mg', '비타비백정 100mg/병', '놀텍정 10mg', '자누메트정 50/850mg', '큐시드정 31.5mg/PTP', '아모잘탄정 5/100mg', '세비카정 10/40mg', '트윈스타정 40/5mg', '카나브정 60mg', '울트라셋이알서방정', '졸로푸트정 100mg', '트라젠타정', '비모보정 500/20mg', '레일라정', '리바로정 4mg', '렉사프로정 15mg', '트라젠타듀오정 2.5/850mg', '낙소졸정 500/20mg', '아질렉트정', '자누메트엑스알서방정 100/1000mg', '글리아타민연질캡슐', '신바로정', '에스원엠프정 20mg', '브린텔릭스정 20mg', '글리틴정', '제미메트서방정 50/1000mg', '아토젯정 10/40mg', '로수젯정10/5밀리그램', '로수바미브정 10/20mg', '카발린캡슐 25mg', '케이캡정 50mg']\n"
     ]
    }
   ],
   "source": [
    "# 1. 'category_id'와 'class_name' 컬럼으로 고유한 쌍을 찾고, ID 기준으로 정렬\n",
    "class_mapping_df = (\n",
    "    train_df[[\"category_id\", \"class_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"category_id\")\n",
    ")\n",
    "# 2. 정렬된 DataFrame에서 클래스 이름만 리스트로 추출\n",
    "sorted_class_names = [\n",
    "    name.split(\"(\")[0].strip() for name in class_mapping_df[\"class_name\"]\n",
    "]\n",
    "# 3. 맨 앞에 'background' 추가\n",
    "classes = [\"background\"] + sorted_class_names\n",
    "\n",
    "print(f\"총 클래스 개수 (배경 포함): {len(classes)}\")\n",
    "print(f\"클래스 리스트: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a9ddb",
   "metadata": {},
   "source": [
    "## 2. Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e73b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 증강 (Augmentation) : Albumentations 라이브러리 사용\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        # PyTorch 텐서로 변환\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"albumentations\", label_fields=[\"labels\"]),\n",
    ")  # bbox 형식은 pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"albumentations\", label_fields=[\"labels\"]),\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# DataLoader를 위한 collate_fn. 이미지와 타겟을 리스트로 묶어줌\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9addecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PillDataset(Dataset):\n",
    "    # --- mode 파라미터 추가 및 df를 직접 받도록 수정 ---\n",
    "    def __init__(self, df, image_dir, mode=\"train\", transforms=None):\n",
    "        self.df = df\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # --- image_ids를 미리 뽑아 중복을 제거 ---\n",
    "        # df['file_name']을 사용하면 이미지 파일 이름으로 고유한 이미지를 식별 가능.\n",
    "        self.image_ids = self.df[\"file_name\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        # --- 고유한 이미지의 개수를 반환 ---\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = self.image_dir / image_id\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Error: Could not load image at path: {image_path}\"\n",
    "            )\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        if self.mode in [\"train\", \"val\"]:\n",
    "            records = self.df[self.df[\"file_name\"] == image_id]\n",
    "            boxes = records[[\"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]].values\n",
    "\n",
    "            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "            labels = records[\"label_idx\"].values\n",
    "\n",
    "            # print(f\"\\n[DEBUG 1] Image: {image_id}, Original Pixel Coords:\\n{boxes}\")\n",
    "\n",
    "            # 바운딩 박스 좌표 정규화\n",
    "            boxes = boxes.astype(np.float32)\n",
    "            boxes[:, [0, 2]] /= w\n",
    "            boxes[:, [1, 3]] /= h\n",
    "\n",
    "            # print(f\"[DEBUG 2] Normalized Coords for Albumentations:\\n{boxes}\")\n",
    "\n",
    "            if self.transforms:\n",
    "                try:\n",
    "                    transformed = self.transforms(\n",
    "                        image=image, bboxes=boxes, labels=labels\n",
    "                    )\n",
    "                    image = transformed[\"image\"]\n",
    "                    boxes = transformed[\"bboxes\"]\n",
    "                    labels = transformed[\"labels\"]\n",
    "                except Exception as e:\n",
    "                    print(f\"!!!!!!!!!!!!!! Albumentations에서 에러 발생 !!!!!!!!!!!!!!\")\n",
    "                    print(f\"Image: {image_id}\")\n",
    "                    print(f\"Boxes sent to transform: {boxes}\")\n",
    "                    # raise e  # 에러를 다시 발생시켜서 멈추게 함\n",
    "\n",
    "            # ... 이하 코드는 이전과 동일 ...\n",
    "            _, new_h, new_w = image.shape\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, [0, 2]] *= new_w\n",
    "                boxes[:, [1, 3]] *= new_h\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            }\n",
    "\n",
    "            if len(target[\"boxes\"]) == 0:\n",
    "                target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "            # 테스트 모드일 경우, 이미지와 파일 이름만 반환\n",
    "        elif self.mode == \"test\":\n",
    "            # 테스트 시에는 보통 기본적인 리사이즈, 정규화만 적용\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(image=image)\n",
    "                image = transformed[\"image\"]\n",
    "\n",
    "            # 나중에 예측 결과를 이미지와 매칭시키기 위해 파일 이름을 반환\n",
    "            return image, image_id\n",
    "\n",
    "\n",
    "# 참고: Subset을 사용할 때 transform을 다르게 적용하려면 약간의 트릭이 필요.\n",
    "# 먼저 transform이 없는 전체 데이터셋을 만듦.\n",
    "# 각 Subset에 맞는 transform을 적용하는 Wrapper 클래스 생성\n",
    "# class TransformSubset(Dataset):\n",
    "#     def __init__(self, subset, transforms):\n",
    "#         self.subset = subset\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image, target = self.subset[idx]\n",
    "\n",
    "#         # NumPy 배열로 변환 (Albumentations 입력 형식)\n",
    "#         boxes = target[\"boxes\"].numpy()\n",
    "#         labels = target[\"labels\"].numpy()\n",
    "\n",
    "#         if self.transforms:\n",
    "#             transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "#             image = transformed[\"image\"]\n",
    "#             target[\"boxes\"] = torch.as_tensor(\n",
    "#                 transformed[\"bboxes\"], dtype=torch.float32\n",
    "#             )\n",
    "#             # 증강 후 bbox가 사라졌을 경우 처리\n",
    "#             if len(target[\"boxes\"]) == 0:\n",
    "#                 target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "#         return image, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405ed05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# pre-trained 모델 로드\n",
    "# Faster R-CNN\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# MobileNetV3\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=\"DEFAULT\"\n",
    ")\n",
    "\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "\n",
    "# 분류기의 입력 피처 수를 가져옴\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# pre-trained head를 새로운 head로 교체\n",
    "# num_classes에 배경(background) 클래스 1개를 더해줘야 함\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83102d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# 재현성을 위해 모든 난수 생성기의 시드를 고정하는 함수.\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d647faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soohyun/miniconda3/envs/JSH/lib/python3.13/site-packages/sklearn/model_selection/_split.py:1035: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating weights for WeightedRandomSampler...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# 1. 데이터 준비\n",
    "df = pd.read_csv(SAVE_PATH)\n",
    "\n",
    "# StratifiedGroupKFold를 위한 데이터 준비\n",
    "groups = df[\"file_name\"]  # 그룹 기준: 이미지 파일 이름\n",
    "labels = df[\"category_id\"]  # 층화 기준: 원본 클래스 ID\n",
    "\n",
    "# K-Fold 설정 (5-fold, 즉 80% train / 20% val)\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# 첫 번째 fold의 train/val 인덱스를 가져옴\n",
    "train_idxs, val_idxs = next(cv.split(df, labels, groups))\n",
    "# 인덱스를 사용해서 데이터프레임을 먼저 분할!\n",
    "train_df_split = df.iloc[train_idxs].reset_index(drop=True)\n",
    "val_df_split = df.iloc[val_idxs].reset_index(drop=True)\n",
    "\n",
    "# --- 각 샘플(이미지)에 대한 가중치 계산 ---\n",
    "print(\"\\nCalculating weights for WeightedRandomSampler...\")\n",
    "\n",
    "# train_df에 있는 각 클래스(label_idx)의 개수를 계산\n",
    "class_counts = train_df[\"label_idx\"].value_counts()\n",
    "\n",
    "# 각 클래스에 대한 가중치를 계산 (클래스 빈도의 역수)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# train_df의 각 행(어노테이션)에 해당하는 가중치를 계산\n",
    "sample_weights = train_df[\"label_idx\"].map(class_weights)\n",
    "\n",
    "# --- 이제 각 '이미지'의 가중치를 계산 (이미지 내 가장 희귀한 클래스 기준) ---\n",
    "# 각 이미지(file_name)에 포함된 어노테이션들의 가중치 중 가장 큰 값을 해당 이미지의 가중치로 설정\n",
    "image_weights_map = (\n",
    "    train_df.groupby(\"file_name\")[\"label_idx\"]\n",
    "    .apply(lambda x: sample_weights.loc[x.index].max())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# train_dataset에서 사용할 최종 가중치 리스트 생성\n",
    "# train_dataset.image_ids 순서와 정확히 일치해야 함\n",
    "train_dataset = PillDataset(\n",
    "    df=train_df_split,\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    mode=\"train\",\n",
    "    transforms=train_transforms,\n",
    ")\n",
    "\n",
    "sampler_weights = [image_weights_map[img_id] for img_id in train_dataset.image_ids]\n",
    "\n",
    "# --- 2. WeightedRandomSampler 생성 ---\n",
    "# replacement=True는 오버샘플링(데이터를 중복해서 뽑는 것)을 허용\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sampler_weights, num_samples=len(sampler_weights), replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = PillDataset(\n",
    "    df=val_df_split,\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    mode=\"val\",\n",
    "    transforms=val_transforms,  # val_transforms 사용\n",
    ")\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({\"file_name\": os.listdir(TEST_IMAGE_DIR)})\n",
    "\n",
    "test_dataset = PillDataset(\n",
    "    df=test_df,\n",
    "    image_dir=TEST_IMAGE_DIR,\n",
    "    mode=\"test\",\n",
    "    transforms=test_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "# --- Data Loader ---\n",
    "\n",
    "# --- 3. DataLoader에 Sampler 적용 ---\n",
    "# sampler를 사용할 때는 shuffle=False로 설정해야 함!\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Sampler가 샘플링을 담당하므로 shuffle은 꺼야 함\n",
    "    collate_fn=collate_fn,\n",
    "    sampler=sampler,  # 여기에 생성한 샘플러를 전달\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1478840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, image_id = next(iter(train_loader))\n",
    "# image_id[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37de70ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[268.0656,  97.2000, 484.7213, 185.6000],\n",
       "         [ 92.8525,  93.6000, 198.8197, 198.0000],\n",
       "         [337.3115, 338.4000, 436.9836, 414.4000]]),\n",
       " 'labels': tensor([56, 73,  1])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, image_id = next(iter(val_loader))\n",
    "image_id[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572d7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=LEARNING_RATE,\n",
    "#     momentum=MOMENTUM,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS, eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac0dbb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 후 검증 점수가 향상되지 않으면 학습을 조기 중단시킵니다.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience=7,\n",
    "        verbose=False,\n",
    "        delta=0,\n",
    "        mode=\"min\",\n",
    "        path=\"../experiments/best_model.pt\",\n",
    "        evaluation_name=\"score\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 검증 점수가 향상된 후 기다릴 에폭 수.\n",
    "            verbose (bool): True일 경우, 점수가 향상될 때마다 메시지를 출력.\n",
    "            delta (float): 점수 향상으로 인정될 최소 변화량.\n",
    "            mode (str): 'min' 또는 'max'. min은 점수가 낮아지는 것을, max는 높아지는 것을 목표로 함.\n",
    "            path (str): 모델 체크포인트 저장 경로.\n",
    "            evaluation_name (str): 로그에 표시될 평가 지표의 이름.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.evaluation_name = evaluation_name\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.inf\n",
    "        else:\n",
    "            self.val_score = -np.inf\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "\n",
    "        # mode에 따른 점수 비교\n",
    "        is_best = False\n",
    "        if self.mode == \"min\":\n",
    "            if (\n",
    "                score < self.best_score - self.delta\n",
    "                if self.best_score is not None\n",
    "                else True\n",
    "            ):\n",
    "                is_best = True\n",
    "        else:  # mode == 'max'\n",
    "            if (\n",
    "                score > self.best_score + self.delta\n",
    "                if self.best_score is not None\n",
    "                else True\n",
    "            ):\n",
    "                is_best = True\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "        elif is_best:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, score, model):\n",
    "        \"\"\"Saves model when validation score improves.\"\"\"\n",
    "        if self.verbose:\n",
    "            # mode에 따라 'decreased' 또는 'increased'를 동적으로 표현\n",
    "            change_text = \"decreased\" if self.mode == \"min\" else \"increased\"\n",
    "            print(\n",
    "                f\"{self.evaluation_name} {change_text} ({self.val_score:.6f} --> {score:.6f}). Saving model ...\"\n",
    "            )\n",
    "\n",
    "        save_dir = os.path.dirname(self.path)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            print(f\"Created directory: {save_dir}\")\n",
    "\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1023bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.0.0.weight: requires_grad=False\n",
      "backbone.body.1.block.0.0.weight: requires_grad=False\n",
      "backbone.body.1.block.1.0.weight: requires_grad=False\n",
      "backbone.body.2.block.0.0.weight: requires_grad=False\n",
      "backbone.body.2.block.1.0.weight: requires_grad=False\n",
      "backbone.body.2.block.2.0.weight: requires_grad=False\n",
      "backbone.body.3.block.0.0.weight: requires_grad=False\n",
      "backbone.body.3.block.1.0.weight: requires_grad=False\n",
      "backbone.body.3.block.2.0.weight: requires_grad=False\n",
      "backbone.body.4.block.0.0.weight: requires_grad=False\n",
      "backbone.body.4.block.1.0.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.4.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.4.block.3.0.weight: requires_grad=False\n",
      "backbone.body.5.block.0.0.weight: requires_grad=False\n",
      "backbone.body.5.block.1.0.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.5.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.5.block.3.0.weight: requires_grad=False\n",
      "backbone.body.6.block.0.0.weight: requires_grad=False\n",
      "backbone.body.6.block.1.0.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.6.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.6.block.3.0.weight: requires_grad=False\n",
      "backbone.body.7.block.0.0.weight: requires_grad=True\n",
      "backbone.body.7.block.1.0.weight: requires_grad=True\n",
      "backbone.body.7.block.2.0.weight: requires_grad=True\n",
      "backbone.body.8.block.0.0.weight: requires_grad=True\n",
      "backbone.body.8.block.1.0.weight: requires_grad=True\n",
      "backbone.body.8.block.2.0.weight: requires_grad=True\n",
      "backbone.body.9.block.0.0.weight: requires_grad=True\n",
      "backbone.body.9.block.1.0.weight: requires_grad=True\n",
      "backbone.body.9.block.2.0.weight: requires_grad=True\n",
      "backbone.body.10.block.0.0.weight: requires_grad=True\n",
      "backbone.body.10.block.1.0.weight: requires_grad=True\n",
      "backbone.body.10.block.2.0.weight: requires_grad=True\n",
      "backbone.body.11.block.0.0.weight: requires_grad=True\n",
      "backbone.body.11.block.1.0.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.11.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.11.block.3.0.weight: requires_grad=True\n",
      "backbone.body.12.block.0.0.weight: requires_grad=True\n",
      "backbone.body.12.block.1.0.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.12.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.12.block.3.0.weight: requires_grad=True\n",
      "backbone.body.13.block.0.0.weight: requires_grad=True\n",
      "backbone.body.13.block.1.0.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.13.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.13.block.3.0.weight: requires_grad=True\n",
      "backbone.body.14.block.0.0.weight: requires_grad=True\n",
      "backbone.body.14.block.1.0.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.14.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.14.block.3.0.weight: requires_grad=True\n",
      "backbone.body.15.block.0.0.weight: requires_grad=True\n",
      "backbone.body.15.block.1.0.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.15.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.15.block.3.0.weight: requires_grad=True\n",
      "backbone.body.16.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.0.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.0.0.bias: requires_grad=True\n",
      "backbone.fpn.inner_blocks.1.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.1.0.bias: requires_grad=True\n",
      "backbone.fpn.layer_blocks.0.0.weight: requires_grad=True\n",
      "backbone.fpn.layer_blocks.0.0.bias: requires_grad=True\n",
      "backbone.fpn.layer_blocks.1.0.weight: requires_grad=True\n",
      "backbone.fpn.layer_blocks.1.0.bias: requires_grad=True\n",
      "rpn.head.conv.0.0.weight: requires_grad=True\n",
      "rpn.head.conv.0.0.bias: requires_grad=True\n",
      "rpn.head.cls_logits.weight: requires_grad=True\n",
      "rpn.head.cls_logits.bias: requires_grad=True\n",
      "rpn.head.bbox_pred.weight: requires_grad=True\n",
      "rpn.head.bbox_pred.bias: requires_grad=True\n",
      "roi_heads.box_head.fc6.weight: requires_grad=True\n",
      "roi_heads.box_head.fc6.bias: requires_grad=True\n",
      "roi_heads.box_head.fc7.weight: requires_grad=True\n",
      "roi_heads.box_head.fc7.bias: requires_grad=True\n",
      "roi_heads.box_predictor.cls_score.weight: requires_grad=True\n",
      "roi_heads.box_predictor.cls_score.bias: requires_grad=True\n",
      "roi_heads.box_predictor.bbox_pred.weight: requires_grad=True\n",
      "roi_heads.box_predictor.bbox_pred.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/60]: 100%|██████████| 41/41 [01:21<00:00,  1.99s/it, loss=4.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP increased (-inf --> 0.007744). Saving model ...\n",
      "Train Loss: 4.5256, Val Loss: 3.9497, mAP@50: 0.0077, LR: 1e-05\n",
      "COCO mAP: 0.0041,  mAP@75: 0.0031, avg_iou: 0.7012606508591596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/60]: 100%|██████████| 41/41 [03:05<00:00,  4.53s/it, loss=3.94]\n"
     ]
    }
   ],
   "source": [
    "# 3. 학습 루프\n",
    "MODEL_NAME = \"fasterrcnn_map50\"\n",
    "\n",
    "\n",
    "print(\"--- Start Training ---\")\n",
    "metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True).to(DEVICE)\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode=\"max\",\n",
    "    evaluation_name=\"Validation mAP\",\n",
    "    path=\"../experiments/\" + MODEL_NAME + \".pt\",\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ===================================\n",
    "    #  Training Step\n",
    "    # ===================================\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "    for images, targets in loop:\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        current_avg_loss = running_loss / (loop.n + 1)\n",
    "        loop.set_postfix(loss=current_avg_loss)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # =====================================\n",
    "    #  Validation Step (✨ 여기가 핵심 수정)\n",
    "    # =====================================\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_iou = 0\n",
    "    tp_count = 0\n",
    "\n",
    "    metric.reset()\n",
    "    # (2) Validation phase\n",
    "    for images, targets in val_loader:\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        # 1. mAP 계산을 위한 예측 (그래디언트 계산 불필요)\n",
    "        with torch.no_grad():  # 예측 부분만 no_grad로 감싸기\n",
    "            predictions = model(images)\n",
    "\n",
    "        # 2. Metric 업데이트\n",
    "        metric.update(predictions, targets)\n",
    "\n",
    "        # IoU 평가 메트릭\n",
    "        for i in range(len(predictions)):\n",
    "            pred_boxes = predictions[i][\"boxes\"]\n",
    "            gt_boxes = targets[i][\"boxes\"]\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # 각 GT 박스에 대해 가장 IoU가 높은 예측 박스를 찾음\n",
    "            iou_matrix = box_iou(pred_boxes, gt_boxes)\n",
    "            max_iou_per_gt, _ = iou_matrix.max(dim=0)\n",
    "\n",
    "            # IoU가 0.5 이상인 경우 (TP)만 계산에 포함\n",
    "            true_positives = max_iou_per_gt[max_iou_per_gt > 0.5]\n",
    "\n",
    "            if len(true_positives) > 0:\n",
    "                total_iou += true_positives.sum().item()\n",
    "                tp_count += len(true_positives)\n",
    "\n",
    "        # 3. Validation Loss 계산 (그래디언트 계산 필요)\n",
    "        #    torch.no_grad() 블록 바깥에서 계산\n",
    "        model.train()  # Loss 계산을 위해 잠시 train 모드로\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        val_loss += losses.item()\n",
    "        model.eval()  # 다음 배치를 위해 다시 eval 모드로 복귀\n",
    "\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    avg_iou = total_iou / tp_count if tp_count > 0 else 0.0\n",
    "\n",
    "    # mAP 평가\n",
    "    mAP_dict = metric.compute()\n",
    "    # Learning Rate Scheduler 적용. val_loss를 고려하여 learning rate를 조정\n",
    "\n",
    "    # mAP 기반EarlyStopping 로직 호출\n",
    "    early_stopping(mAP_dict[\"map_50\"], model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # scheduler 설정\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    loop.write(\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, mAP@50: {mAP_dict['map_50']:.4f}, LR: {current_lr:.0e}\"\n",
    "    )\n",
    "    loop.write(\n",
    "        f\"COCO mAP: {mAP_dict['map']:.4f},  mAP@75: {mAP_dict['map_75']:.4f}, avg_iou: {avg_iou}\"\n",
    "    )\n",
    "    # loop.write(\n",
    "    #     f\"Val_Cls: {val_losses['loss_classifier']:.4f}, Val_Box: {val_losses['loss_box_reg']:.4f} | \"\n",
    "    # )\n",
    "    # 메모리 정리\n",
    "    gc.collect()  # 남아 있는 텐서 객체를 명시적으로 정리\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "print(\"--- Finish Training ---\")\n",
    "# 학습 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label=\"train loss\", color=\"blue\")\n",
    "plt.plot(val_losses, label=\"val loss\", color=\"orange\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve: \" + MODEL_NAME)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "loss_plot_path = os.path.join(EXPERIMENT_DIR, f\"{MODEL_NAME}_loss_curve.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.show()\n",
    "# 최종 모델 저장\n",
    "# torch.save(model.state_dict(), f\"{EXPERIMENT_DIR}/final_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
