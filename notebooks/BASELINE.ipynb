{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbe8d54",
   "metadata": {},
   "source": [
    "이 파일에다가 작업하지 마시고 각자 복사해서 써주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ee864",
   "metadata": {},
   "source": [
    "## 0. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de11a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# Configuration 설정\n",
    "# ====================================================================\n",
    "# 하이퍼파라미터 및 경로 등 실험에 필요한 설정들을 모아둠\n",
    "# 실험 추적 및 재현성을 위해 모든 값은 여기에서 수정하고자 함\n",
    "import os  # 디렉토리, 파일 경로 조작 등\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path  # payhon path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2  # OpenCV - 고급 이미지/비디오 처리\n",
    "import torch\n",
    "from torch.utils.data import Dataset  # 커스텀 데이터셋, 배치 로딩\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Garbage Collector 모듈\n",
    "import gc\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "# 메모리 정리 루틴\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- 디바이스 설정 ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# --- 학습 하이퍼파라미터 ---\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# --- 데이터 경로 설정 ---\n",
    "# DATA_ROOT = path\n",
    "# DATA_ROOT = \"data/raw/ai03-level1-project/\"  # 경로\n",
    "# TRAIN_IMAGE_DIR = os.path.join(DATA_ROOT, \"train_images\")\n",
    "# TRAIN_ANNO_DIR = os.path.join(DATA_ROOT, \"train_annotations\")\n",
    "# TEST_IMAGE_DIR = os.path.join(DATA_ROOT, \"test_images\")\n",
    "# PROCESSED_TRAIN_CSV = \"../data/processed/train_df.csv\"  # 데이터 전처리된 csv 파일\n",
    "\n",
    "# --- 모델 설정 ---\n",
    "NUM_CLASSES = 73\n",
    "MODEL_NAME = \"fasterrcnn_resnet50_fpn\"\n",
    "USE_PRETRAINED = True\n",
    "\n",
    "# --- 학습 고도화 설정 ---\n",
    "USE_SCHEDULER = True  # Learning rate scheduler 사용 여부\n",
    "EARLY_STOPPING = True  # Early stopping 적용 여부\n",
    "AUGMENTATION = True  # 데이터 증강 사용 여부\n",
    "\n",
    "# --- 실험 로깅용 설정 ---\n",
    "USE_WANDB = True\n",
    "WANDB_PROJECT = \"AI03-Project-1\"\n",
    "RUN_NAME = f\"{MODEL_NAME}_bs{BATCH_SIZE}_lr{LEARNING_RATE}\"\n",
    "\n",
    "\n",
    "# --- 실험 결과 저장 경로 ---\n",
    "EXPERIMENT_DIR = \"../experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ac65f",
   "metadata": {},
   "source": [
    "## 1. Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85626da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_annotations(ann_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    복잡한 3중 폴더 구조의 원본 어노테이션을 파싱하여\n",
    "    하나의 Pandas DataFrame으로 반환하는 함수.\n",
    "    \"\"\"\n",
    "    all_annotations = []\n",
    "\n",
    "    # Level 1: 이미지별 폴더 순회\n",
    "    image_level_dirs = os.listdir(ann_dir)\n",
    "    for image_dir_name in tqdm(image_level_dirs, desc=\"[L1] Images\"):\n",
    "        image_dir_path = ann_dir / image_dir_name\n",
    "        if not image_dir_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        # Level 2: 알약 종류 폴더 순회\n",
    "        pill_level_dirs = os.listdir(image_dir_path)\n",
    "        for pill_dir_name in pill_level_dirs:\n",
    "            pill_dir_path = image_dir_path / pill_dir_name\n",
    "            if not pill_dir_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Level 3: 실제 .json 파일 파싱\n",
    "            json_files = [f for f in os.listdir(pill_dir_path) if f.endswith(\".json\")]\n",
    "            if not json_files:\n",
    "                continue\n",
    "\n",
    "            # 첫 번째 json 파일만 사용\n",
    "            json_file_path = pill_dir_path / json_files[0]\n",
    "\n",
    "            try:\n",
    "                with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    ann_data = json.load(f)\n",
    "\n",
    "                    image_info = ann_data.get(\"images\", [{}])[0]\n",
    "                    annotation_info = ann_data.get(\"annotations\", [{}])[0]\n",
    "                    category_info = ann_data.get(\"categories\", [{}])[0]\n",
    "\n",
    "                    all_annotations.append(\n",
    "                        {\n",
    "                            \"image_id\": image_info.get(\"id\"),\n",
    "                            \"file_name\": image_info.get(\"file_name\"),\n",
    "                            \"width\": image_info.get(\"width\"),\n",
    "                            \"height\": image_info.get(\"height\"),\n",
    "                            \"category_id\": category_info.get(\"id\"),\n",
    "                            \"class_name\": category_info.get(\"name\"),\n",
    "                            \"bbox\": annotation_info.get(\"bbox\"),\n",
    "                        }\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n파일 처리 에러: {json_file_path}, 에러: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e807cce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_raw_annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m TEST_IMAGE_DIR = RAW_DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mtest_images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m SAVE_PATH = PROCESSED_DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mtrain_df.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_df = \u001b[43mparse_raw_annotations\u001b[49m(TRAIN_ANNO_DIR)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- (1). bbox 컬럼을 4개로 분리 ---\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# bbox 컬럼 분리\u001b[39;00m\n\u001b[32m     16\u001b[39m bbox_df = pd.DataFrame(\n\u001b[32m     17\u001b[39m     train_df[\u001b[33m\"\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m\"\u001b[39m].tolist(), columns=[\u001b[33m\"\u001b[39m\u001b[33mbbox_x\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbbox_y\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbbox_w\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbbox_h\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'parse_raw_annotations' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. 핵심 함수를 호출해서 DataFrame 생성\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"../\")  # 상대경로\n",
    "RAW_DATA_DIR = BASE_DIR / \"data/raw/ai03-level1-project\"\n",
    "PROCESSED_DATA_DIR = BASE_DIR / \"data/processed\"\n",
    "\n",
    "TRAIN_IMAGE_DIR = RAW_DATA_DIR / \"train_images\"\n",
    "TRAIN_ANNO_DIR = RAW_DATA_DIR / \"train_annotations\"\n",
    "TEST_IMAGE_DIR = RAW_DATA_DIR / \"test_images\"\n",
    "SAVE_PATH = PROCESSED_DATA_DIR / \"train_df.csv\"\n",
    "train_df = parse_raw_annotations(TRAIN_ANNO_DIR)\n",
    "\n",
    "# --- (1). bbox 컬럼을 4개로 분리 ---\n",
    "# bbox 컬럼 분리\n",
    "bbox_df = pd.DataFrame(\n",
    "    train_df[\"bbox\"].tolist(), columns=[\"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]\n",
    ")\n",
    "train_df = pd.concat([train_df.drop(\"bbox\", axis=1), bbox_df], axis=1)\n",
    "\n",
    "# ✨ --- [핵심 수정] 잘못된 바운딩 박스 데이터 제거 ---\n",
    "# xmax (bbox_x + bbox_w)가 이미지 너비(width)를 초과하는 경우\n",
    "invalid_x = train_df[\"bbox_x\"] + train_df[\"bbox_w\"] > train_df[\"width\"]\n",
    "# ymax (bbox_y + bbox_h)가 이미지 높이(height)를 초과하는 경우\n",
    "invalid_y = train_df[\"bbox_y\"] + train_df[\"bbox_h\"] > train_df[\"height\"]\n",
    "\n",
    "# 잘못된 데이터를 필터링\n",
    "invalid_rows = train_df[invalid_x | invalid_y]\n",
    "if not invalid_rows.empty:\n",
    "    print(f\"--- {len(invalid_rows)}개의 잘못된 바운딩 박스 데이터를 찾았습니다. ---\")\n",
    "    print(\n",
    "        invalid_rows[\n",
    "            [\"file_name\", \"width\", \"height\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 유효한 데이터만 남김\n",
    "    train_df = train_df[~(invalid_x | invalid_y)]\n",
    "    print(f\"\\n잘못된 데이터를 제거하고, {len(train_df)}개의 데이터만 사용합니다.\")\n",
    "\n",
    "# --- (2). category_id를 새로운 label_idx로 매핑 ---\n",
    "# 고유한 category_id 목록을 뽑아 정렬\n",
    "unique_category_ids = sorted(train_df[\"category_id\"].unique())\n",
    "NUM_CLASSES = len(unique_category_ids)\n",
    "# category_id를 0, 1, 2... 인덱스로 변환하는 딕셔너리 생성\n",
    "id_to_idx = {\n",
    "    int(original_id): idx\n",
    "    for idx, original_id in enumerate(\n",
    "        unique_category_ids, start=1\n",
    "    )  # <--- start=1 추가!\n",
    "}\n",
    "# 이 매핑 정보를 사용해서 'label_idx'라는 새 컬럼을 추가\n",
    "train_df[\"label_idx\"] = train_df[\"category_id\"].map(id_to_idx)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# 나중에 추론 결과에서 원래 클래스 이름을 찾을 수 있도록 매핑 정보도 저장\n",
    "label_map = {\n",
    "    \"id_to_idx\": id_to_idx,\n",
    "    \"idx_to_id\": {idx: int(original_id) for original_id, idx in id_to_idx.items()},\n",
    "    \"id_to_name\": dict(zip(train_df[\"category_id\"], train_df[\"class_name\"])),\n",
    "}\n",
    "with open(PROCESSED_DATA_DIR / \"label_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n총 {len(unique_category_ids)}개의 고유 클래스를 발견했습니다.\")\n",
    "print(\"라벨 매핑 정보를 'data/processed/label_map.json'에 저장했습니다.\")\n",
    "\n",
    "\n",
    "# 3. 최종 DataFrame을 CSV 파일로 저장\n",
    "train_df.to_csv(SAVE_PATH, index=False)\n",
    "\n",
    "print(f\"\\n--- 데이터 전처리 및 저장 완료! ---\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190b5136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label_idx'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label_idx\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8437147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name\n",
       "기넥신에프정(은행엽엑스)(수출용)    206\n",
       "일양하이트린정 2mg            98\n",
       "뮤테란캡슐 100mg            74\n",
       "보령부스파정 5mg             74\n",
       "동아가바펜틴정 800mg          58\n",
       "                     ... \n",
       "쿠에타핀정 25mg              4\n",
       "렉사프로정 15mg              4\n",
       "브린텔릭스정 20mg             4\n",
       "졸로푸트정 100mg             4\n",
       "자이프렉사정 2.5mg            4\n",
       "Name: count, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"class_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8f2cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['뮤테란캡슐 100mg', '큐시드정 31.5mg/PTP', '트루비타정 60mg/병',\n",
       "       '써스펜8시간이알서방정 650mg', '글리틴정(콜린알포세레이트)', '일양하이트린정 2mg',\n",
       "       '로수바미브정 10/20mg', '제미메트서방정 50/1000mg', '노바스크정 5mg',\n",
       "       '기넥신에프정(은행엽엑스)(수출용)', '아토젯정 10/40mg', '트라젠타듀오정 2.5/850mg',\n",
       "       '리피토정 20mg', '동아가바펜틴정 800mg', '울트라셋이알서방정', '보령부스파정 5mg',\n",
       "       '케이캡정 50mg', '자누비아정 50mg', '로수젯정10/5밀리그램', '아모잘탄정 5/100mg',\n",
       "       '트라젠타정(리나글립틴)', '크레스토정 20mg', '가바토파정 100mg', '란스톤엘에프디티정 30mg',\n",
       "       '펠루비정(펠루비프로펜)', '콜리네이트연질캡슐 400mg', '자누메트정 50/850mg', '카나브정 60mg',\n",
       "       '플라빅스정 75mg', '리바로정 4mg', '글리아타민연질캡슐', '리피로우정 20mg',\n",
       "       '트윈스타정 40/5mg', '엑스포지정 5/160mg', '아토르바정 10mg', '카발린캡슐 25mg',\n",
       "       '에빅사정(메만틴염산염)(비매품)', '다보타민큐정 10mg/병', '삼남건조수산화알루미늄겔정',\n",
       "       '리렉스펜정 300mg/PTP', '타이레놀이알서방정(아세트아미노펜)(수출용)', '삐콤씨에프정 618.6mg/병',\n",
       "       '쎄로켈정 100mg', '무코스타정(레바미피드)(비매품)', '낙소졸정 500/20mg',\n",
       "       '종근당글리아티린연질캡슐(콜린알포세레이트)\\xa0', '오마코연질캡슐(오메가-3-산에틸에스테르90)',\n",
       "       '리리카캡슐 150mg', '에스원엠프정 20mg', '레일라정', '세비카정 10/40mg', '스토가정 10mg',\n",
       "       '알드린정', '메가파워정 90mg/병', '신바로정', '놀텍정 10mg', '비타비백정 100mg/병',\n",
       "       '맥시부펜이알정 300mg', '타이레놀정500mg', '자누메트엑스알서방정 100/1000mg',\n",
       "       '뉴로메드정(옥시라세탐)', '마도파정', '조인스정 200mg', '아빌리파이정 10mg',\n",
       "       '비모보정 500/20mg', '쿠에타핀정 25mg', '에어탈정(아세클로페낙)', '브린텔릭스정 20mg',\n",
       "       '렉사프로정 15mg', '아질렉트정(라사길린메실산염)', '라비에트정 20mg', '졸로푸트정 100mg',\n",
       "       '자이프렉사정 2.5mg'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_tmp = train_df[\"class_name\"].unique()\n",
    "classes_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02840906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 클래스 개수 (배경 포함): 74\n",
      "클래스 리스트: ['background', '보령부스파정 5mg', '뮤테란캡슐 100mg', '일양하이트린정 2mg', '기넥신에프정', '무코스타정', '알드린정', '뉴로메드정', '타이레놀정500mg', '에어탈정', '삼남건조수산화알루미늄겔정', '타이레놀이알서방정', '삐콤씨에프정 618.6mg/병', '조인스정 200mg', '쎄로켈정 100mg', '리렉스펜정 300mg/PTP', '아빌리파이정 10mg', '자이프렉사정 2.5mg', '다보타민큐정 10mg/병', '써스펜8시간이알서방정 650mg', '에빅사정', '리피토정 20mg', '크레스토정 20mg', '가바토파정 100mg', '동아가바펜틴정 800mg', '오마코연질캡슐', '란스톤엘에프디티정 30mg', '리리카캡슐 150mg', '종근당글리아티린연질캡슐', '콜리네이트연질캡슐 400mg', '트루비타정 60mg/병', '스토가정 10mg', '노바스크정 5mg', '마도파정', '플라빅스정 75mg', '엑스포지정 5/160mg', '펠루비정', '아토르바정 10mg', '라비에트정 20mg', '리피로우정 20mg', '자누비아정 50mg', '맥시부펜이알정 300mg', '메가파워정 90mg/병', '쿠에타핀정 25mg', '비타비백정 100mg/병', '놀텍정 10mg', '자누메트정 50/850mg', '큐시드정 31.5mg/PTP', '아모잘탄정 5/100mg', '세비카정 10/40mg', '트윈스타정 40/5mg', '카나브정 60mg', '울트라셋이알서방정', '졸로푸트정 100mg', '트라젠타정', '비모보정 500/20mg', '레일라정', '리바로정 4mg', '렉사프로정 15mg', '트라젠타듀오정 2.5/850mg', '낙소졸정 500/20mg', '아질렉트정', '자누메트엑스알서방정 100/1000mg', '글리아타민연질캡슐', '신바로정', '에스원엠프정 20mg', '브린텔릭스정 20mg', '글리틴정', '제미메트서방정 50/1000mg', '아토젯정 10/40mg', '로수젯정10/5밀리그램', '로수바미브정 10/20mg', '카발린캡슐 25mg', '케이캡정 50mg']\n"
     ]
    }
   ],
   "source": [
    "# 1. 'category_id'와 'class_name' 컬럼으로 고유한 쌍을 찾고, ID 기준으로 정렬\n",
    "class_mapping_df = (\n",
    "    train_df[[\"category_id\", \"class_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"category_id\")\n",
    ")\n",
    "# 2. 정렬된 DataFrame에서 클래스 이름만 리스트로 추출\n",
    "sorted_class_names = [\n",
    "    name.split(\"(\")[0].strip() for name in class_mapping_df[\"class_name\"]\n",
    "]\n",
    "# 3. 맨 앞에 'background' 추가\n",
    "classes = [\"background\"] + sorted_class_names\n",
    "\n",
    "print(f\"총 클래스 개수 (배경 포함): {len(classes)}\")\n",
    "print(f\"클래스 리스트: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a9ddb",
   "metadata": {},
   "source": [
    "## 2. Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e73b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 증강 (Augmentation) : Albumentations 라이브러리 사용\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        # PyTorch 텐서로 변환\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"albumentations\", label_fields=[\"labels\"]),\n",
    ")  # bbox 형식은 pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"albumentations\", label_fields=[\"labels\"]),\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        A.pytorch.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# DataLoader를 위한 collate_fn. 이미지와 타겟을 리스트로 묶어줌\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9addecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PillDataset(Dataset):\n",
    "    # --- mode 파라미터 추가 및 df를 직접 받도록 수정 ---\n",
    "    def __init__(self, df, image_dir, mode=\"train\", transforms=None):\n",
    "        self.df = df\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # --- image_ids를 미리 뽑아 중복을 제거 ---\n",
    "        # df['file_name']을 사용하면 이미지 파일 이름으로 고유한 이미지를 식별 가능.\n",
    "        self.image_ids = self.df[\"file_name\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        # --- 고유한 이미지의 개수를 반환 ---\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = self.image_dir / image_id\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Error: Could not load image at path: {image_path}\"\n",
    "            )\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        if self.mode in [\"train\", \"val\"]:\n",
    "            records = self.df[self.df[\"file_name\"] == image_id]\n",
    "            boxes = records[[\"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]].values\n",
    "\n",
    "            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "            labels = records[\"label_idx\"].values\n",
    "\n",
    "            # print(f\"\\n[DEBUG 1] Image: {image_id}, Original Pixel Coords:\\n{boxes}\")\n",
    "\n",
    "            # 바운딩 박스 좌표 정규화\n",
    "            boxes = boxes.astype(np.float32)\n",
    "            boxes[:, [0, 2]] /= w\n",
    "            boxes[:, [1, 3]] /= h\n",
    "\n",
    "            # print(f\"[DEBUG 2] Normalized Coords for Albumentations:\\n{boxes}\")\n",
    "\n",
    "            if self.transforms:\n",
    "                try:\n",
    "                    transformed = self.transforms(\n",
    "                        image=image, bboxes=boxes, labels=labels\n",
    "                    )\n",
    "                    image = transformed[\"image\"]\n",
    "                    boxes = transformed[\"bboxes\"]\n",
    "                    labels = transformed[\"labels\"]\n",
    "                except Exception as e:\n",
    "                    print(f\"!!!!!!!!!!!!!! Albumentations에서 에러 발생 !!!!!!!!!!!!!!\")\n",
    "                    print(f\"Image: {image_id}\")\n",
    "                    print(f\"Boxes sent to transform: {boxes}\")\n",
    "                    # raise e  # 에러를 다시 발생시켜서 멈추게 함\n",
    "\n",
    "            # ... 이하 코드는 이전과 동일 ...\n",
    "            _, new_h, new_w = image.shape\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, [0, 2]] *= new_w\n",
    "                boxes[:, [1, 3]] *= new_h\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            }\n",
    "\n",
    "            if len(target[\"boxes\"]) == 0:\n",
    "                target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "            # 테스트 모드일 경우, 이미지와 파일 이름만 반환\n",
    "        elif self.mode == \"test\":\n",
    "            # 테스트 시에는 보통 기본적인 리사이즈, 정규화만 적용\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(image=image)\n",
    "                image = transformed[\"image\"]\n",
    "\n",
    "            # 나중에 예측 결과를 이미지와 매칭시키기 위해 파일 이름을 반환\n",
    "            return image, image_id\n",
    "\n",
    "\n",
    "# 참고: Subset을 사용할 때 transform을 다르게 적용하려면 약간의 트릭이 필요.\n",
    "# 먼저 transform이 없는 전체 데이터셋을 만듦.\n",
    "# 각 Subset에 맞는 transform을 적용하는 Wrapper 클래스 생성\n",
    "# class TransformSubset(Dataset):\n",
    "#     def __init__(self, subset, transforms):\n",
    "#         self.subset = subset\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image, target = self.subset[idx]\n",
    "\n",
    "#         # NumPy 배열로 변환 (Albumentations 입력 형식)\n",
    "#         boxes = target[\"boxes\"].numpy()\n",
    "#         labels = target[\"labels\"].numpy()\n",
    "\n",
    "#         if self.transforms:\n",
    "#             transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "#             image = transformed[\"image\"]\n",
    "#             target[\"boxes\"] = torch.as_tensor(\n",
    "#                 transformed[\"bboxes\"], dtype=torch.float32\n",
    "#             )\n",
    "#             # 증강 후 bbox가 사라졌을 경우 처리\n",
    "#             if len(target[\"boxes\"]) == 0:\n",
    "#                 target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "#         return image, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405ed05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# pre-trained 모델 로드\n",
    "# Faster R-CNN\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# MobileNetV3\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=\"DEFAULT\"\n",
    ")\n",
    "\n",
    "\n",
    "# 분류기의 입력 피처 수를 가져옴\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# pre-trained head를 새로운 head로 교체\n",
    "# num_classes에 배경(background) 클래스 1개를 더해줘야 함\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83102d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# 재현성을 위해 모든 난수 생성기의 시드를 고정하는 함수.\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d647faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soohyun/ls/envs/JSH/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1023: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# 1. 데이터 준비\n",
    "df = pd.read_csv(SAVE_PATH)\n",
    "\n",
    "# StratifiedGroupKFold를 위한 데이터 준비\n",
    "groups = df[\"file_name\"]  # 그룹 기준: 이미지 파일 이름\n",
    "labels = df[\"category_id\"]  # 층화 기준: 원본 클래스 ID\n",
    "\n",
    "# K-Fold 설정 (5-fold, 즉 80% train / 20% val)\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# 첫 번째 fold의 train/val 인덱스를 가져옴\n",
    "train_idxs, val_idxs = next(cv.split(df, labels, groups))\n",
    "# 1. 인덱스를 사용해서 데이터프레임을 먼저 분할!\n",
    "train_df_split = df.iloc[train_idxs].reset_index(drop=True)\n",
    "val_df_split = df.iloc[val_idxs].reset_index(drop=True)\n",
    "\n",
    "# 2. 분할된 데이터프레임으로 각각 Dataset 생성 (Subset, TransformSubset 불필요!)\n",
    "train_dataset = PillDataset(\n",
    "    df=train_df_split,\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    mode=\"train\",\n",
    "    transforms=train_transforms,\n",
    ")\n",
    "\n",
    "val_dataset = PillDataset(\n",
    "    df=val_df_split,\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    mode=\"val\",\n",
    "    transforms=val_transforms,  # val_transforms 사용\n",
    ")\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({\"file_name\": os.listdir(TEST_IMAGE_DIR)})\n",
    "\n",
    "test_dataset = PillDataset(\n",
    "    df=test_df,\n",
    "    image_dir=TEST_IMAGE_DIR,\n",
    "    mode=\"test\",\n",
    "    transforms=test_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "# --- Data Loader ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1478840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, image_id = next(iter(train_loader))\n",
    "# image_id[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37de70ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[ 78.1639,  73.2000, 187.8033, 154.0000],\n",
       "         [ 72.9180, 310.8000, 196.7213, 400.0000]]),\n",
       " 'labels': tensor([ 5, 23])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, image_id = next(iter(val_loader))\n",
    "image_id[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572d7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=LEARNING_RATE,\n",
    "#     momentum=MOMENTUM,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS, eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1023bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.0.0.weight: requires_grad=False\n",
      "backbone.body.1.block.0.0.weight: requires_grad=False\n",
      "backbone.body.1.block.1.0.weight: requires_grad=False\n",
      "backbone.body.2.block.0.0.weight: requires_grad=False\n",
      "backbone.body.2.block.1.0.weight: requires_grad=False\n",
      "backbone.body.2.block.2.0.weight: requires_grad=False\n",
      "backbone.body.3.block.0.0.weight: requires_grad=False\n",
      "backbone.body.3.block.1.0.weight: requires_grad=False\n",
      "backbone.body.3.block.2.0.weight: requires_grad=False\n",
      "backbone.body.4.block.0.0.weight: requires_grad=False\n",
      "backbone.body.4.block.1.0.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.4.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.4.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.4.block.3.0.weight: requires_grad=False\n",
      "backbone.body.5.block.0.0.weight: requires_grad=False\n",
      "backbone.body.5.block.1.0.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.5.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.5.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.5.block.3.0.weight: requires_grad=False\n",
      "backbone.body.6.block.0.0.weight: requires_grad=False\n",
      "backbone.body.6.block.1.0.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc1.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc1.bias: requires_grad=False\n",
      "backbone.body.6.block.2.fc2.weight: requires_grad=False\n",
      "backbone.body.6.block.2.fc2.bias: requires_grad=False\n",
      "backbone.body.6.block.3.0.weight: requires_grad=False\n",
      "backbone.body.7.block.0.0.weight: requires_grad=True\n",
      "backbone.body.7.block.1.0.weight: requires_grad=True\n",
      "backbone.body.7.block.2.0.weight: requires_grad=True\n",
      "backbone.body.8.block.0.0.weight: requires_grad=True\n",
      "backbone.body.8.block.1.0.weight: requires_grad=True\n",
      "backbone.body.8.block.2.0.weight: requires_grad=True\n",
      "backbone.body.9.block.0.0.weight: requires_grad=True\n",
      "backbone.body.9.block.1.0.weight: requires_grad=True\n",
      "backbone.body.9.block.2.0.weight: requires_grad=True\n",
      "backbone.body.10.block.0.0.weight: requires_grad=True\n",
      "backbone.body.10.block.1.0.weight: requires_grad=True\n",
      "backbone.body.10.block.2.0.weight: requires_grad=True\n",
      "backbone.body.11.block.0.0.weight: requires_grad=True\n",
      "backbone.body.11.block.1.0.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.11.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.11.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.11.block.3.0.weight: requires_grad=True\n",
      "backbone.body.12.block.0.0.weight: requires_grad=True\n",
      "backbone.body.12.block.1.0.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.12.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.12.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.12.block.3.0.weight: requires_grad=True\n",
      "backbone.body.13.block.0.0.weight: requires_grad=True\n",
      "backbone.body.13.block.1.0.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.13.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.13.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.13.block.3.0.weight: requires_grad=True\n",
      "backbone.body.14.block.0.0.weight: requires_grad=True\n",
      "backbone.body.14.block.1.0.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.14.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.14.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.14.block.3.0.weight: requires_grad=True\n",
      "backbone.body.15.block.0.0.weight: requires_grad=True\n",
      "backbone.body.15.block.1.0.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc1.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc1.bias: requires_grad=True\n",
      "backbone.body.15.block.2.fc2.weight: requires_grad=True\n",
      "backbone.body.15.block.2.fc2.bias: requires_grad=True\n",
      "backbone.body.15.block.3.0.weight: requires_grad=True\n",
      "backbone.body.16.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.0.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.0.0.bias: requires_grad=True\n",
      "backbone.fpn.inner_blocks.1.0.weight: requires_grad=True\n",
      "backbone.fpn.inner_blocks.1.0.bias: requires_grad=True\n",
      "backbone.fpn.layer_blocks.0.0.weight: requires_grad=True\n",
      "backbone.fpn.layer_blocks.0.0.bias: requires_grad=True\n",
      "backbone.fpn.layer_blocks.1.0.weight: requires_grad=True\n",
      "backbone.fpn.layer_blocks.1.0.bias: requires_grad=True\n",
      "rpn.head.conv.0.0.weight: requires_grad=True\n",
      "rpn.head.conv.0.0.bias: requires_grad=True\n",
      "rpn.head.cls_logits.weight: requires_grad=True\n",
      "rpn.head.cls_logits.bias: requires_grad=True\n",
      "rpn.head.bbox_pred.weight: requires_grad=True\n",
      "rpn.head.bbox_pred.bias: requires_grad=True\n",
      "roi_heads.box_head.fc6.weight: requires_grad=True\n",
      "roi_heads.box_head.fc6.bias: requires_grad=True\n",
      "roi_heads.box_head.fc7.weight: requires_grad=True\n",
      "roi_heads.box_head.fc7.bias: requires_grad=True\n",
      "roi_heads.box_predictor.cls_score.weight: requires_grad=True\n",
      "roi_heads.box_predictor.cls_score.bias: requires_grad=True\n",
      "roi_heads.box_predictor.bbox_pred.weight: requires_grad=True\n",
      "roi_heads.box_predictor.bbox_pred.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786c978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: 100%|██████████| 40/40 [00:24<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.5431, Val Loss: 3.0239, Validation mAP: 0.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]: 100%|██████████| 40/40 [00:25<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4924, Val Loss: 2.1710, Validation mAP: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]: 100%|██████████| 40/40 [00:25<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8783, Val Loss: 1.8365, Validation mAP: 0.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]: 100%|██████████| 40/40 [00:25<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6144, Val Loss: 1.5749, Validation mAP: 0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]: 100%|██████████| 40/40 [00:26<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4462, Val Loss: 1.4022, Validation mAP: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]: 100%|██████████| 40/40 [00:25<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3164, Val Loss: 1.3250, Validation mAP: 0.7396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2078, Val Loss: 1.2555, Validation mAP: 0.7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]: 100%|██████████| 40/40 [00:23<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1227, Val Loss: 1.1729, Validation mAP: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]: 100%|██████████| 40/40 [00:24<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0340, Val Loss: 1.0910, Validation mAP: 0.7782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9989, Val Loss: 1.0832, Validation mAP: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9424, Val Loss: 1.0209, Validation mAP: 0.7753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8643, Val Loss: 0.9780, Validation mAP: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8422, Val Loss: 0.9545, Validation mAP: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7859, Val Loss: 0.9428, Validation mAP: 0.7784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]: 100%|██████████| 40/40 [00:25<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7548, Val Loss: 0.9124, Validation mAP: 0.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]: 100%|██████████| 40/40 [00:20<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6942, Val Loss: 0.9009, Validation mAP: 0.7791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6938, Val Loss: 0.8774, Validation mAP: 0.7825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]: 100%|██████████| 40/40 [00:18<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6582, Val Loss: 0.8660, Validation mAP: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6562, Val Loss: 0.8632, Validation mAP: 0.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6449, Val Loss: 0.8587, Validation mAP: 0.7799\n",
      "--- Finish Training ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 학습 루프\n",
    "print(\"--- Start Training ---\")\n",
    "metric = MeanAveragePrecision(box_format=\"xyxy\").to(DEVICE)\n",
    "# early_stopping = EarlyStopping(patience=7, verbose=True, path=path_model)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ===================================\n",
    "    #  Training Step\n",
    "    # ===================================\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "    for images, targets in loop:\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # =====================================\n",
    "    #  Validation Step (✨ 여기가 핵심 수정)\n",
    "    # =====================================\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    metric.reset()\n",
    "    # (2) Validation phase\n",
    "    for images, targets in val_loader:\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        # 1. mAP 계산을 위한 예측 (그래디언트 계산 불필요)\n",
    "        with torch.no_grad():  # 👈✨ 예측 부분만 no_grad로 감싸기\n",
    "            predictions = model(images)\n",
    "\n",
    "        # 2. Metric 업데이트\n",
    "        metric.update(predictions, targets)\n",
    "\n",
    "        # 3. Validation Loss 계산 (그래디언트 계산 필요)\n",
    "        #    torch.no_grad() 블록 바깥에서 계산\n",
    "        model.train()  # Loss 계산을 위해 잠시 train 모드로\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        val_loss += losses.item()\n",
    "        model.eval()  # 다음 배치를 위해 다시 eval 모드로 복귀\n",
    "\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    # mAP 평가\n",
    "    mAP_dict = metric.compute()\n",
    "    # mAP = evaluate_model(all_predictions, all_ground_truths, classes)\n",
    "    print(\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, Validation mAP: {mAP_dict['map_50']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"--- Finish Training ---\"),\n",
    "# 최종 모델 저장\n",
    "# torch.save(model.state_dict(), f\"{EXPERIMENT_DIR}/final_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
